#!/usr/bin/env python3
"""
Mock vLLM/Ollama æœåŠ¡å™¨
ç”¨äºæµ‹è¯• AgentBus æœ¬åœ°æ¨¡å‹é›†æˆ,æ— éœ€çœŸå®å®‰è£… vLLM æˆ– Ollama
"""
import asyncio
import json
from datetime import datetime
from fastapi import FastAPI, Request
from fastapi.responses import StreamingResponse, JSONResponse
import uvicorn
from pydantic import BaseModel
from typing import Optional, List

app = FastAPI(title="Mock vLLM Server", version="1.0.0")

# æ¨¡æ‹Ÿçš„æ¨¡å‹åˆ—è¡¨
MOCK_MODELS = [
    "qwen3_32B",
    "tinyllama",
    "phi-2",
    "gemma-2b"
]

class ChatMessage(BaseModel):
    role: str
    content: str

class ChatCompletionRequest(BaseModel):
    model: str
    messages: List[ChatMessage]
    temperature: Optional[float] = 0.7
    max_tokens: Optional[int] = 2048
    stream: Optional[bool] = False

class CompletionRequest(BaseModel):
    model: str
    prompt: str
    temperature: Optional[float] = 0.7
    max_tokens: Optional[int] = 2048
    stream: Optional[bool] = False

# Mock å“åº”ç”Ÿæˆå™¨
def generate_mock_response(prompt: str, model: str) -> str:
    """ç”Ÿæˆæ¨¡æ‹Ÿçš„ AI å“åº”"""
    responses = {
        "ä½ å¥½": f"ä½ å¥½!æˆ‘æ˜¯ {model} æ¨¡å‹ã€‚å¾ˆé«˜å…´ä¸ºæ‚¨æœåŠ¡!",
        "1+1": "1+1ç­‰äº2ã€‚è¿™æ˜¯ä¸€ä¸ªç®€å•çš„æ•°å­¦é—®é¢˜ã€‚",
        "ä»‹ç»": f"æˆ‘æ˜¯ {model},ä¸€ä¸ªè¿è¡Œåœ¨æœ¬åœ°çš„å¤§è¯­è¨€æ¨¡å‹ã€‚æˆ‘å¯ä»¥å¸®åŠ©æ‚¨å®Œæˆå„ç§ä»»åŠ¡,åŒ…æ‹¬å›ç­”é—®é¢˜ã€ç¼–å†™ä»£ç ã€ç¿»è¯‘æ–‡æœ¬ç­‰ã€‚",
        "default": f"[Mock {model}] æˆ‘æ”¶åˆ°äº†æ‚¨çš„æ¶ˆæ¯: {prompt[:50]}... è¿™æ˜¯ä¸€ä¸ªæ¨¡æ‹Ÿå“åº”,ç”¨äºæµ‹è¯•æœ¬åœ°æ¨¡å‹é›†æˆã€‚"
    }
    
    # ç®€å•çš„å…³é”®è¯åŒ¹é…
    for key, response in responses.items():
        if key in prompt.lower():
            return response
    
    return responses["default"]

async def stream_response(text: str):
    """æµå¼è¿”å›å“åº”"""
    words = text.split()
    for i, word in enumerate(words):
        chunk = {
            "id": f"chatcmpl-mock-{i}",
            "object": "chat.completion.chunk",
            "created": int(datetime.now().timestamp()),
            "model": "mock-model",
            "choices": [{
                "index": 0,
                "delta": {"content": word + " "},
                "finish_reason": None if i < len(words) - 1 else "stop"
            }]
        }
        yield f"data: {json.dumps(chunk)}\n\n"
        await asyncio.sleep(0.05)  # æ¨¡æ‹Ÿç”Ÿæˆå»¶è¿Ÿ
    
    yield "data: [DONE]\n\n"

@app.get("/")
async def root():
    """æ ¹è·¯å¾„"""
    return {
        "message": "Mock vLLM/Ollama Server",
        "status": "running",
        "models": MOCK_MODELS
    }

@app.get("/v1/models")
async def list_models():
    """åˆ—å‡ºå¯ç”¨æ¨¡å‹ (OpenAI å…¼å®¹)"""
    return {
        "object": "list",
        "data": [
            {
                "id": model,
                "object": "model",
                "created": int(datetime.now().timestamp()),
                "owned_by": "mock-vllm"
            }
            for model in MOCK_MODELS
        ]
    }

@app.get("/api/tags")
async def ollama_list_models():
    """åˆ—å‡ºå¯ç”¨æ¨¡å‹ (Ollama å…¼å®¹)"""
    return {
        "models": [
            {
                "name": model,
                "modified_at": datetime.now().isoformat(),
                "size": 1000000000,  # 1GB
                "digest": "mock-digest",
                "details": {
                    "format": "gguf",
                    "family": "llama",
                    "parameter_size": "7B"
                }
            }
            for model in MOCK_MODELS
        ]
    }

@app.post("/v1/chat/completions")
async def chat_completions(request: ChatCompletionRequest):
    """èŠå¤©è¡¥å…¨ (OpenAI å…¼å®¹)"""
    # è·å–æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯
    user_message = next(
        (msg.content for msg in reversed(request.messages) if msg.role == "user"),
        "Hello"
    )
    
    response_text = generate_mock_response(user_message, request.model)
    
    if request.stream:
        return StreamingResponse(
            stream_response(response_text),
            media_type="text/event-stream"
        )
    
    return {
        "id": "chatcmpl-mock",
        "object": "chat.completion",
        "created": int(datetime.now().timestamp()),
        "model": request.model,
        "choices": [{
            "index": 0,
            "message": {
                "role": "assistant",
                "content": response_text
            },
            "finish_reason": "stop"
        }],
        "usage": {
            "prompt_tokens": len(user_message.split()),
            "completion_tokens": len(response_text.split()),
            "total_tokens": len(user_message.split()) + len(response_text.split())
        }
    }

@app.post("/v1/completions")
async def completions(request: CompletionRequest):
    """æ–‡æœ¬è¡¥å…¨ (OpenAI å…¼å®¹)"""
    response_text = generate_mock_response(request.prompt, request.model)
    
    if request.stream:
        return StreamingResponse(
            stream_response(response_text),
            media_type="text/event-stream"
        )
    
    return {
        "id": "cmpl-mock",
        "object": "text_completion",
        "created": int(datetime.now().timestamp()),
        "model": request.model,
        "choices": [{
            "text": response_text,
            "index": 0,
            "finish_reason": "stop"
        }],
        "usage": {
            "prompt_tokens": len(request.prompt.split()),
            "completion_tokens": len(response_text.split()),
            "total_tokens": len(request.prompt.split()) + len(response_text.split())
        }
    }

@app.post("/api/generate")
async def ollama_generate(request: Request):
    """ç”Ÿæˆå“åº” (Ollama å…¼å®¹)"""
    data = await request.json()
    model = data.get("model", "tinyllama")
    prompt = data.get("prompt", "")
    stream = data.get("stream", False)
    
    response_text = generate_mock_response(prompt, model)
    
    if stream:
        async def ollama_stream():
            words = response_text.split()
            for word in words:
                chunk = {
                    "model": model,
                    "created_at": datetime.now().isoformat(),
                    "response": word + " ",
                    "done": False
                }
                yield json.dumps(chunk) + "\n"
                await asyncio.sleep(0.05)
            
            final = {
                "model": model,
                "created_at": datetime.now().isoformat(),
                "response": "",
                "done": True,
                "total_duration": 1000000000,
                "load_duration": 100000000,
                "prompt_eval_count": len(prompt.split()),
                "eval_count": len(response_text.split())
            }
            yield json.dumps(final) + "\n"
        
        return StreamingResponse(ollama_stream(), media_type="application/x-ndjson")
    
    return {
        "model": model,
        "created_at": datetime.now().isoformat(),
        "response": response_text,
        "done": True,
        "total_duration": 1000000000,
        "load_duration": 100000000,
        "prompt_eval_count": len(prompt.split()),
        "eval_count": len(response_text.split())
    }

@app.post("/api/chat")
async def ollama_chat(request: Request):
    """èŠå¤© (Ollama å…¼å®¹)"""
    data = await request.json()
    model = data.get("model", "tinyllama")
    messages = data.get("messages", [])
    stream = data.get("stream", False)
    
    user_message = next(
        (msg["content"] for msg in reversed(messages) if msg["role"] == "user"),
        "Hello"
    )
    
    response_text = generate_mock_response(user_message, model)
    
    if stream:
        async def ollama_chat_stream():
            words = response_text.split()
            for word in words:
                chunk = {
                    "model": model,
                    "created_at": datetime.now().isoformat(),
                    "message": {
                        "role": "assistant",
                        "content": word + " "
                    },
                    "done": False
                }
                yield json.dumps(chunk) + "\n"
                await asyncio.sleep(0.05)
            
            final = {
                "model": model,
                "created_at": datetime.now().isoformat(),
                "message": {
                    "role": "assistant",
                    "content": ""
                },
                "done": True
            }
            yield json.dumps(final) + "\n"
        
        return StreamingResponse(ollama_chat_stream(), media_type="application/x-ndjson")
    
    return {
        "model": model,
        "created_at": datetime.now().isoformat(),
        "message": {
            "role": "assistant",
            "content": response_text
        },
        "done": True
    }

@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    return {
        "status": "healthy",
        "service": "mock-vllm",
        "models_loaded": len(MOCK_MODELS)
    }

if __name__ == "__main__":
    print("=" * 60)
    print("ğŸš€ Mock vLLM/Ollama æœåŠ¡å™¨å¯åŠ¨ä¸­...")
    print("=" * 60)
    print()
    print("ğŸ“ æœåŠ¡åœ°å€:")
    print("   - OpenAI å…¼å®¹: http://localhost:8030/v1/chat/completions")
    print("   - Ollama å…¼å®¹: http://localhost:11434/api/generate")
    print()
    print("ğŸ“‹ å¯ç”¨æ¨¡å‹:")
    for model in MOCK_MODELS:
        print(f"   - {model}")
    print()
    print("ğŸ§ª æµ‹è¯•å‘½ä»¤:")
    print("   curl http://localhost:8030/v1/models")
    print("   curl http://localhost:11434/api/tags")
    print()
    print("=" * 60)
    
    # åŒæ—¶åœ¨ä¸¤ä¸ªç«¯å£å¯åŠ¨
    import threading
    
    def run_vllm():
        uvicorn.run(app, host="0.0.0.0", port=8030, log_level="info")
    
    def run_ollama():
        uvicorn.run(app, host="0.0.0.0", port=11434, log_level="info")
    
    # å¯åŠ¨ vLLM ç«¯å£
    vllm_thread = threading.Thread(target=run_vllm, daemon=True)
    vllm_thread.start()
    
    # ä¸»çº¿ç¨‹è¿è¡Œ Ollama ç«¯å£
    try:
        run_ollama()
    except KeyboardInterrupt:
        print("\nğŸ‘‹ æœåŠ¡å™¨å·²åœæ­¢")
