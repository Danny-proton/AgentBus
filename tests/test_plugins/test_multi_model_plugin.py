#!/usr/bin/env python3
"""
å¤šæ¨¡å‹åè°ƒå™¨æ’ä»¶æµ‹è¯•
Multi-Model Coordinator Plugin Tests

æµ‹è¯•å¤šæ¨¡å‹åè°ƒå™¨æ’ä»¶çš„å„é¡¹åŠŸèƒ½ï¼ŒåŒ…æ‹¬ï¼š
- æ’ä»¶æ¿€æ´»å’Œåœç”¨
- æ¨¡å‹ç®¡ç†å·¥å…·
- ä»»åŠ¡å¤„ç†å·¥å…·
- é’©å­ç³»ç»Ÿ
- å‘½ä»¤ç³»ç»Ÿ
- ç»Ÿè®¡å’Œç›‘æ§åŠŸèƒ½
"""

import asyncio
import pytest
import logging
from unittest.mock import AsyncMock, MagicMock, patch
from typing import Dict, Any

from agentbus.plugins import PluginContext, PluginManager
from agentbus.plugins.multi_model_plugin import MultiModelPlugin
from agentbus.services.multi_model_coordinator import (
    ModelConfig, TaskRequest, TaskType, TaskPriority, ModelType
)


class TestMultiModelPlugin:
    """å¤šæ¨¡å‹åè°ƒå™¨æ’ä»¶æµ‹è¯•ç±»"""
    
    @pytest.fixture
    def plugin_context(self):
        """åˆ›å»ºæµ‹è¯•ç”¨çš„æ’ä»¶ä¸Šä¸‹æ–‡"""
        config = {
            'default_models': [],
            'fusion_strategy': 'best',
            'max_concurrent_tasks': 10,
            'enable_monitoring': True
        }
        
        logger = logging.getLogger('test_plugin')
        logger.setLevel(logging.DEBUG)
        
        runtime = {
            'test_mode': True
        }
        
        return PluginContext(
            config=config,
            logger=logger,
            runtime=runtime
        )
    
    @pytest.fixture
    def plugin(self, plugin_context):
        """åˆ›å»ºæµ‹è¯•ç”¨çš„æ’ä»¶å®ä¾‹"""
        return MultiModelPlugin("test_multi_model_plugin", plugin_context)
    
    @pytest.fixture
    def sample_model_config(self):
        """åˆ›å»ºç¤ºä¾‹æ¨¡å‹é…ç½®"""
        return ModelConfig(
            model_id="test-gpt-4",
            model_name="Test GPT-4",
            model_type=ModelType.TEXT_GENERATION,
            provider="openai",
            capabilities=[TaskType.TEXT_GENERATION, TaskType.QUESTION_ANSWERING],
            cost_per_token=0.00003,
            quality_score=0.95,
            max_tokens=4096,
            temperature=0.7,
            api_key="test-key"
        )
    
    @pytest.fixture
    def sample_task_request(self):
        """åˆ›å»ºç¤ºä¾‹ä»»åŠ¡è¯·æ±‚"""
        return TaskRequest(
            task_id="test-task-001",
            task_type=TaskType.TEXT_GENERATION,
            content="è¯·å†™ä¸€æ®µå…³äºAIçš„ä»‹ç»",
            priority=TaskPriority.NORMAL,
            required_capabilities=[TaskType.TEXT_GENERATION],
            max_cost=0.01
        )
    
    def test_plugin_initialization(self, plugin):
        """æµ‹è¯•æ’ä»¶åˆå§‹åŒ–"""
        assert plugin.plugin_id == "test_multi_model_plugin"
        assert plugin.coordinator is not None
        assert plugin.plugin_stats['tasks_submitted'] == 0
        assert plugin.plugin_stats['tasks_completed'] == 0
        assert plugin.plugin_stats['models_registered'] == 0
        assert len(plugin.monitored_tasks) == 0
    
    def test_get_info(self, plugin):
        """æµ‹è¯•è·å–æ’ä»¶ä¿¡æ¯"""
        info = plugin.get_info()
        
        assert info['id'] == plugin.plugin_id
        assert info['name'] == 'Multi-Model Coordinator Plugin'
        assert info['version'] == '1.0.0'
        assert 'capabilities' in info
        assert 'config_schema' in info
        assert 'multi_model_coordination' in info['capabilities']
    
    @pytest.mark.asyncio
    async def test_plugin_activation(self, plugin):
        """æµ‹è¯•æ’ä»¶æ¿€æ´»"""
        # æ¨¡æ‹Ÿåè°ƒå™¨åˆå§‹åŒ–æˆåŠŸ
        with patch.object(plugin.coordinator, 'initialize', return_value=True) as mock_init:
            success = await plugin.activate()
            
            assert success is True
            assert plugin.status.value == 'active'
            mock_init.assert_called_once()
            
            # æ£€æŸ¥å·¥å…·æ˜¯å¦æ³¨å†Œ
            tools = plugin.get_tools()
            assert len(tools) > 0
            tool_names = [tool.name for tool in tools]
            assert 'submit_multi_model_task' in tool_names
            assert 'register_model' in tool_names
            assert 'get_task_result' in tool_names
            assert 'list_models' in tool_names
            assert 'get_coordinator_stats' in tool_names
            
            # æ£€æŸ¥é’©å­æ˜¯å¦æ³¨å†Œ
            hooks = plugin.get_hooks()
            assert 'multi_model_task_submitted' in hooks
            assert 'multi_model_task_completed' in hooks
            assert 'model_registered' in hooks
            
            # æ£€æŸ¥å‘½ä»¤æ˜¯å¦æ³¨å†Œ
            commands = plugin.get_commands()
            command_names = [cmd['command'] for cmd in commands]
            assert '/models' in command_names
            assert '/tasks' in command_names
            assert '/stats' in command_names
            assert '/health' in command_names
    
    @pytest.mark.asyncio
    async def test_plugin_activation_failure(self, plugin):
        """æµ‹è¯•æ’ä»¶æ¿€æ´»å¤±è´¥"""
        # æ¨¡æ‹Ÿåè°ƒå™¨åˆå§‹åŒ–å¤±è´¥
        with patch.object(plugin.coordinator, 'initialize', return_value=False):
            with pytest.raises(RuntimeError, match="Failed to initialize"):
                await plugin.activate()
            
            assert plugin.status.value == 'error'
    
    @pytest.mark.asyncio
    async def test_plugin_deactivation(self, plugin):
        """æµ‹è¯•æ’ä»¶åœç”¨"""
        # å…ˆæ¿€æ´»æ’ä»¶
        with patch.object(plugin.coordinator, 'initialize', return_value=True):
            await plugin.activate()
        
        # æ¨¡æ‹Ÿç›‘æ§ä»»åŠ¡å’Œåè°ƒå™¨å…³é—­
        plugin.monitored_tasks['test-task'] = 1234567890
        with patch.object(plugin.coordinator, 'shutdown', return_value=None) as mock_shutdown:
            success = await plugin.deactivate()
            
            assert success is True
            assert plugin.status.value == 'deactivated'
            assert len(plugin.monitored_tasks) == 0
            mock_shutdown.assert_called_once()
    
    @pytest.mark.asyncio
    async def test_submit_multi_model_task(self, plugin):
        """æµ‹è¯•æäº¤å¤šæ¨¡å‹ä»»åŠ¡"""
        await plugin.activate()
        
        # æ¨¡æ‹Ÿåè°ƒå™¨æäº¤ä»»åŠ¡
        test_task_id = "test-task-123"
        with patch.object(plugin.coordinator, 'submit_task', return_value=test_task_id) as mock_submit:
            with patch.object(plugin.coordinator, 'get_available_models', return_value=[]):
                result = await plugin.submit_multi_model_task(
                    task_type="text_generation",
                    content="æµ‹è¯•å†…å®¹",
                    priority="normal"
                )
                
                assert result['success'] is True
                assert result['task_id'] == test_task_id
                assert 'ä»»åŠ¡å·²æˆåŠŸæäº¤' in result['message']
                
                # æ£€æŸ¥ç»Ÿè®¡æ›´æ–°
                assert plugin.plugin_stats['tasks_submitted'] == 1
                
                mock_submit.assert_called_once()
    
    @pytest.mark.asyncio
    async def test_submit_multi_model_task_invalid_type(self, plugin):
        """æµ‹è¯•æäº¤æ— æ•ˆç±»å‹ä»»åŠ¡"""
        await plugin.activate()
        
        result = await plugin.submit_multi_model_task(
            task_type="invalid_type",
            content="æµ‹è¯•å†…å®¹"
        )
        
        assert result['success'] is False
        assert 'æ— æ•ˆçš„ä»»åŠ¡å‚æ•°' in result['error']
        assert result['task_id'] is None
    
    @pytest.mark.asyncio
    async def test_register_model_tool(self, plugin, sample_model_config):
        """æµ‹è¯•æ³¨å†Œæ¨¡å‹å·¥å…·"""
        await plugin.activate()
        
        # æ¨¡æ‹Ÿåè°ƒå™¨æ³¨å†Œæ¨¡å‹
        with patch.object(plugin.coordinator, 'register_model', return_value=True) as mock_register:
            result = plugin.register_model_tool(
                model_id=sample_model_config.model_id,
                model_name=sample_model_config.model_name,
                model_type=sample_model_config.model_type.value,
                provider=sample_model_config.provider,
                capabilities=[cap.value for cap in sample_model_config.capabilities],
                api_key=sample_model_config.api_key,
                cost_per_token=sample_model_config.cost_per_token,
                quality_score=sample_model_config.quality_score
            )
            
            assert result['success'] is True
            assert result['model_id'] == sample_model_config.model_id
            assert 'æ³¨å†ŒæˆåŠŸ' in result['message']
            
            # æ£€æŸ¥ç»Ÿè®¡æ›´æ–°
            assert plugin.plugin_stats['models_registered'] == 1
            
            mock_register.assert_called_once()
    
    def test_register_model_tool_invalid_type(self, plugin):
        """æµ‹è¯•æ³¨å†Œæ— æ•ˆç±»å‹æ¨¡å‹"""
        result = plugin.register_model_tool(
            model_id="test-model",
            model_name="Test Model",
            model_type="invalid_type",
            provider="test",
            capabilities=["text_generation"]
        )
        
        assert result['success'] is False
        assert 'æ— æ•ˆçš„æ¨¡å‹å‚æ•°' in result['error']
        assert result['model_id'] == "test-model"
    
    @pytest.mark.asyncio
    async def test_unregister_model_tool(self, plugin):
        """æµ‹è¯•æ³¨é”€æ¨¡å‹å·¥å…·"""
        await plugin.activate()
        
        # æ¨¡æ‹Ÿåè°ƒå™¨æ³¨é”€æ¨¡å‹
        with patch.object(plugin.coordinator, 'unregister_model', return_value=True) as mock_unregister:
            result = plugin.unregister_model_tool("test-model")
            
            assert result['success'] is True
            assert result['model_id'] == "test-model"
            assert 'æ³¨é”€æˆåŠŸ' in result['message']
            
            mock_unregister.assert_called_once_with("test-model")
    
    @pytest.mark.asyncio
    async def test_get_task_result_tool(self, plugin):
        """æµ‹è¯•è·å–ä»»åŠ¡ç»“æœå·¥å…·"""
        await plugin.activate()
        
        # æ¨¡æ‹Ÿä»»åŠ¡ç»“æœ
        from agentbus.services.multi_model_coordinator import TaskResult, TaskStatus
        
        mock_result = TaskResult(
            task_id="test-task",
            status=TaskStatus.COMPLETED,
            final_content="æµ‹è¯•ç»“æœ",
            total_time=1.5,
            total_cost=0.001,
            fusion_method="best"
        )
        
        with patch.object(plugin.coordinator, 'get_task_result', return_value=mock_result):
            result = await plugin.get_task_result_tool("test-task")
            
            assert result['success'] is True
            assert result['task_id'] == "test-task"
            assert result['status'] == "completed"
            assert result['final_content'] == "æµ‹è¯•ç»“æœ"
            assert result['total_time'] == 1.5
            assert result['total_cost'] == 0.001
    
    @pytest.mark.asyncio
    async def test_get_task_result_not_found(self, plugin):
        """æµ‹è¯•è·å–ä¸å­˜åœ¨çš„ä»»åŠ¡ç»“æœ"""
        await plugin.activate()
        
        with patch.object(plugin.coordinator, 'get_task_result', return_value=None):
            result = await plugin.get_task_result_tool("non-existent-task")
            
            assert result['success'] is False
            assert 'ä»»åŠ¡ä¸å­˜åœ¨' in result['error']
    
    @pytest.mark.asyncio
    async def test_cancel_task_tool(self, plugin):
        """æµ‹è¯•å–æ¶ˆä»»åŠ¡å·¥å…·"""
        await plugin.activate()
        
        # æ·»åŠ ç›‘æ§ä»»åŠ¡
        plugin.monitored_tasks['test-task'] = 1234567890
        
        # æ¨¡æ‹Ÿåè°ƒå™¨å–æ¶ˆä»»åŠ¡
        with patch.object(plugin.coordinator, 'cancel_task', return_value=True):
            result = await plugin.cancel_task_tool("test-task")
            
            assert result['success'] is True
            assert result['task_id'] == "test-task"
            assert 'å–æ¶ˆæˆåŠŸ' in result['message']
            assert 'test-task' not in plugin.monitored_tasks
    
    @pytest.mark.asyncio
    async def test_list_models_tool(self, plugin):
        """æµ‹è¯•åˆ—å‡ºæ¨¡å‹å·¥å…·"""
        await plugin.activate()
        
        # æ¨¡æ‹Ÿæ¨¡å‹åˆ—è¡¨
        mock_models = [
            ModelConfig(
                model_id="test-model-1",
                model_name="Test Model 1",
                model_type=ModelType.TEXT_GENERATION,
                provider="openai",
                capabilities=[TaskType.TEXT_GENERATION]
            ),
            ModelConfig(
                model_id="test-model-2",
                model_name="Test Model 2",
                model_type=ModelType.CODE_GENERATION,
                provider="anthropic",
                capabilities=[TaskType.CODE_GENERATION]
            )
        ]
        
        with patch.object(plugin.coordinator, 'get_available_models', return_value=mock_models):
            result = plugin.list_models_tool()
            
            assert result['success'] is True
            assert result['total_count'] == 2
            assert len(result['models']) == 2
            assert result['models'][0]['model_id'] == "test-model-1"
            assert result['models'][1]['model_id'] == "test-model-2"
    
    @pytest.mark.asyncio
    async def test_list_models_tool_with_filter(self, plugin):
        """æµ‹è¯•å¸¦è¿‡æ»¤æ¡ä»¶çš„åˆ—å‡ºæ¨¡å‹å·¥å…·"""
        await plugin.activate()
        
        # æ¨¡æ‹Ÿè¿‡æ»¤åçš„æ¨¡å‹åˆ—è¡¨
        mock_models = [
            ModelConfig(
                model_id="test-text-model",
                model_name="Test Text Model",
                model_type=ModelType.TEXT_GENERATION,
                provider="openai",
                capabilities=[TaskType.TEXT_GENERATION]
            )
        ]
        
        with patch.object(plugin.coordinator, 'get_available_models', return_value=mock_models):
            result = plugin.list_models_tool(task_type="text_generation")
            
            assert result['success'] is True
            assert result['filtered_by'] == "text_generation"
            assert result['total_count'] == 1
            assert result['models'][0]['model_type'] == "text_generation"
    
    def test_list_models_tool_invalid_type(self, plugin):
        """æµ‹è¯•åˆ—å‡ºæ— æ•ˆç±»å‹æ¨¡å‹"""
        result = plugin.list_models_tool(task_type="invalid_type")
        
        assert result['success'] is False
        assert 'æ— æ•ˆçš„ä»»åŠ¡ç±»å‹' in result['error']
        assert result['models'] == []
        assert result['total_count'] == 0
    
    @pytest.mark.asyncio
    async def test_get_coordinator_stats_tool(self, plugin):
        """æµ‹è¯•è·å–åè°ƒå™¨ç»Ÿè®¡å·¥å…·"""
        await plugin.activate()
        
        # æ¨¡æ‹Ÿåè°ƒå™¨ç»Ÿè®¡
        mock_stats = {
            'active_tasks': 2,
            'total_tasks': 10,
            'completed_tasks': 8,
            'success_rate': 0.8,
            'avg_processing_time': 2.5,
            'avg_cost': 0.001,
            'registered_models': 3,
            'active_models': 3
        }
        
        with patch.object(plugin.coordinator, 'get_coordinator_stats', return_value=mock_stats):
            result = await plugin.get_coordinator_stats_tool()
            
            assert result['success'] is True
            assert 'stats' in result
            assert result['stats']['active_tasks'] == 2
            assert result['stats']['registered_models'] == 3
            assert 'plugin_stats' in result['stats']
    
    def test_get_plugin_stats_tool(self, plugin):
        """æµ‹è¯•è·å–æ’ä»¶ç»Ÿè®¡å·¥å…·"""
        plugin.plugin_stats['tasks_submitted'] = 5
        plugin.plugin_stats['tasks_completed'] = 4
        plugin.monitored_tasks['task1'] = 123
        
        result = plugin.get_plugin_stats_tool()
        
        assert result['success'] is True
        assert result['plugin_id'] == plugin.plugin_id
        assert result['stats']['tasks_submitted'] == 5
        assert result['stats']['tasks_completed'] == 4
        assert result['monitored_tasks'] == 1
        assert result['registered_tools'] > 0
        assert result['registered_hooks'] > 0
        assert result['registered_commands'] > 0
    
    def test_prepare_prompt_tool(self, plugin):
        """æµ‹è¯•å‡†å¤‡æç¤ºè¯å·¥å…·"""
        result = plugin.prepare_prompt_tool(
            task_type="text_generation",
            content="åŸå§‹å†…å®¹"
        )
        
        assert result['success'] is True
        assert result['original_content'] == "åŸå§‹å†…å®¹"
        assert result['task_type'] == "text_generation"
        assert 'prepared_prompt' in result
    
    def test_recommend_models_tool(self, plugin):
        """æµ‹è¯•æ¨¡å‹æ¨èå·¥å…·"""
        # æ¨¡æ‹Ÿå¯ç”¨æ¨¡å‹
        mock_models = [
            ModelConfig(
                model_id="high-quality-model",
                model_name="High Quality Model",
                model_type=ModelType.TEXT_GENERATION,
                provider="openai",
                capabilities=[TaskType.TEXT_GENERATION],
                quality_score=0.95,
                cost_per_token=0.00003
            ),
            ModelConfig(
                model_id="low-cost-model",
                model_name="Low Cost Model",
                model_type=ModelType.TEXT_GENERATION,
                provider="local",
                capabilities=[TaskType.TEXT_GENERATION],
                quality_score=0.75,
                cost_per_token=0.0
            )
        ]
        
        with patch.object(plugin.coordinator, 'get_available_models', return_value=mock_models):
            result = plugin.recommend_models_tool(
                task_type="text_generation",
                max_models=2
            )
            
            assert result['success'] is True
            assert result['task_type'] == "text_generation"
            assert len(result['recommended_models']) == 2
            assert result['recommended_models'][0]['model_id'] == "high-quality-model"  # é«˜è´¨é‡ä¼˜å…ˆ
    
    def test_recommend_models_tool_invalid_type(self, plugin):
        """æµ‹è¯•æ— æ•ˆç±»å‹çš„æ¨¡å‹æ¨è"""
        result = plugin.recommend_models_tool(task_type="invalid_type")
        
        assert result['success'] is False
        assert 'æ— æ•ˆçš„ä»»åŠ¡ç±»å‹' in result['error']
        assert result['recommended_models'] == []
    
    @pytest.mark.asyncio
    async def test_hook_handlers(self, plugin):
        """æµ‹è¯•é’©å­å¤„ç†æ–¹æ³•"""
        # æµ‹è¯•ä»»åŠ¡æäº¤é’©å­
        task_data = {'task_id': 'test-task', 'task_type': 'text_generation'}
        await plugin.on_task_submitted(task_data)  # åº”è¯¥ä¸æŠ¥é”™
        
        # æµ‹è¯•ä»»åŠ¡å®Œæˆé’©å­
        plugin.monitored_tasks['test-task'] = 1234567890
        await plugin.on_task_completed(task_data)
        assert 'test-task' not in plugin.monitored_tasks
        assert plugin.plugin_stats['tasks_completed'] == 1
        
        # æµ‹è¯•ä»»åŠ¡å¤±è´¥é’©å­
        await plugin.on_task_failed(task_data)
        assert plugin.plugin_stats['tasks_failed'] == 1
        
        # æµ‹è¯•æ¨¡å‹æ³¨å†Œé’©å­
        model_data = {'model_id': 'test-model', 'model_name': 'Test Model'}
        await plugin.on_model_registered(model_data)  # åº”è¯¥ä¸æŠ¥é”™
        
        await plugin.on_model_unregistered(model_data)  # åº”è¯¥ä¸æŠ¥é”™
    
    @pytest.mark.asyncio
    async def test_command_handlers(self, plugin):
        """æµ‹è¯•å‘½ä»¤å¤„ç†æ–¹æ³•"""
        await plugin.activate()
        
        # æµ‹è¯•æ¨¡å‹å‘½ä»¤
        result = await plugin.handle_models_command("")
        assert "ğŸ“Š æ¨¡å‹åˆ—è¡¨" in result
        
        # æµ‹è¯•ä»»åŠ¡å‘½ä»¤
        with patch.object(plugin, 'get_coordinator_stats_tool') as mock_stats:
            mock_stats.return_value = {
                'success': True,
                'stats': {
                    'active_tasks': 1,
                    'total_tasks': 5,
                    'success_rate': 0.8,
                    'monitored_tasks': 1,
                    'avg_monitor_time': 2.5
                }
            }
            result = await plugin.handle_tasks_command("")
            assert "ğŸ“‹ ä»»åŠ¡çŠ¶æ€" in result
        
        # æµ‹è¯•ç»Ÿè®¡å‘½ä»¤
        with patch.object(plugin, 'get_plugin_stats_tool') as mock_plugin_stats:
            with patch.object(plugin, 'get_coordinator_stats_tool') as mock_coord_stats:
                mock_plugin_stats.return_value = {
                    'success': True,
                    'stats': {
                        'tasks_submitted': 5,
                        'tasks_completed': 4,
                        'tasks_failed': 1,
                        'models_registered': 3,
                        'total_processing_time': 10.0,
                        'total_cost': 0.005
                    }
                }
                mock_coord_stats.return_value = {
                    'success': True,
                    'stats': {
                        'active_tasks': 1,
                        'avg_processing_time': 2.5,
                        'avg_cost': 0.001,
                        'active_models': 3
                    }
                }
                result = await plugin.handle_stats_command("")
                assert "ğŸ“ˆ ç»Ÿè®¡ä¿¡æ¯" in result
        
        # æµ‹è¯•å¥åº·æ£€æŸ¥å‘½ä»¤
        with patch.object(plugin, 'get_coordinator_stats_tool') as mock_health:
            mock_health.return_value = {
                'success': True,
                'stats': {
                    'active_tasks': 5,
                    'success_rate': 0.9,
                    'registered_models': 3,
                    'active_models': 3
                }
            }
            result = await plugin.handle_health_command("")
            assert "ğŸ’Š å¥åº·æ£€æŸ¥" in result
            assert "ğŸŸ¢ å¥åº·" in result
    
    @pytest.mark.asyncio
    async def test_config_management(self, plugin):
        """æµ‹è¯•é…ç½®ç®¡ç†"""
        # æµ‹è¯•è·å–é…ç½®
        assert plugin.get_config('enable_monitoring', False) is True
        assert plugin.get_config('non_existent', 'default') == 'default'
        
        # æµ‹è¯•è®¾ç½®é…ç½®
        plugin.set_config('test_key', 'test_value')
        assert plugin.get_config('test_key') == 'test_value'
    
    @pytest.mark.asyncio
    async def test_runtime_management(self, plugin):
        """æµ‹è¯•è¿è¡Œæ—¶å˜é‡ç®¡ç†"""
        # æµ‹è¯•è·å–è¿è¡Œæ—¶å˜é‡
        assert plugin.get_runtime('test_mode') is True
        assert plugin.get_runtime('non_existent', 'default') == 'default'
        
        # æµ‹è¯•è®¾ç½®è¿è¡Œæ—¶å˜é‡
        plugin.set_runtime('test_runtime_key', 'test_runtime_value')
        assert plugin.get_runtime('test_runtime_key') == 'test_runtime_value'
    
    @pytest.mark.asyncio
    async def test_plugin_integration(self, plugin):
        """æµ‹è¯•æ’ä»¶é›†æˆåŠŸèƒ½"""
        await plugin.activate()
        
        # æ³¨å†Œä¸€ä¸ªæ¨¡å‹
        register_result = plugin.register_model_tool(
            model_id="integration-test-model",
            model_name="Integration Test Model",
            model_type="text_generation",
            provider="test",
            capabilities=["text_generation"]
        )
        assert register_result['success'] is True
        
        # æäº¤ä¸€ä¸ªä»»åŠ¡
        task_result = await plugin.submit_multi_model_task(
            task_type="text_generation",
            content="é›†æˆæµ‹è¯•å†…å®¹"
        )
        assert task_result['success'] is True
        
        # è·å–æ’ä»¶ç»Ÿè®¡
        stats_result = plugin.get_plugin_stats_tool()
        assert stats_result['success'] is True
        assert stats_result['stats']['models_registered'] >= 1
        assert stats_result['stats']['tasks_submitted'] >= 1
    
    @pytest.mark.asyncio
    async def test_plugin_error_handling(self, plugin):
        """æµ‹è¯•æ’ä»¶é”™è¯¯å¤„ç†"""
        await plugin.activate()
        
        # æµ‹è¯•ä¸å­˜åœ¨çš„å·¥å…·
        with pytest.raises(ValueError, match="Tool 'non_existent_tool' not found"):
            await plugin.execute_tool('non_existent_tool')
        
        # æµ‹è¯•é’©å­è§¦å‘é”™è¯¯
        with patch.object(plugin, 'get_hooks', return_value={'test_event': [MagicMock(handler=lambda x: 1/0)]}):
            await plugin._trigger_hook('test_event', {})
            # é”™è¯¯åº”è¯¥è¢«è®°å½•ä½†ä¸åº”è¯¥æŠ›å‡ºå¼‚å¸¸
    
    @pytest.mark.asyncio
    async def test_plugin_string_representation(self, plugin):
        """æµ‹è¯•æ’ä»¶å­—ç¬¦ä¸²è¡¨ç¤º"""
        str_repr = str(plugin)
        assert "AgentBusPlugin" in str_repr
        assert plugin.plugin_id in str_repr
        
        repr_str = repr(plugin)
        assert "AgentBusPlugin" in repr_str
        assert plugin.plugin_id in repr_str
        assert "tools=" in repr_str
        assert "hooks=" in repr_str


class TestPluginIntegration:
    """æ’ä»¶é›†æˆæµ‹è¯•ç±»"""
    
    @pytest.mark.asyncio
    async def test_plugin_manager_integration(self):
        """æµ‹è¯•æ’ä»¶ç®¡ç†å™¨é›†æˆ"""
        # åˆ›å»ºæ’ä»¶ç®¡ç†å™¨
        manager = PluginManager()
        
        # åˆ›å»ºæ’ä»¶ä¸Šä¸‹æ–‡
        context = PluginContext(
            config={'test_mode': True},
            logger=logging.getLogger('test'),
            runtime={}
        )
        
        # åˆ›å»ºæ’ä»¶å®ä¾‹
        plugin = MultiModelPlugin("integration_test_plugin", context)
        
        # æ¿€æ´»æ’ä»¶
        success = await plugin.activate()
        assert success is True
        
        # æ£€æŸ¥æ’ä»¶çŠ¶æ€
        assert plugin.status.value == 'active'
        
        # è·å–æ’ä»¶ä¿¡æ¯
        info = plugin.get_info()
        assert info['name'] == 'Multi-Model Coordinator Plugin'
        
        # åœç”¨æ’ä»¶
        success = await plugin.deactivate()
        assert success is True
    
    @pytest.mark.asyncio
    async def test_plugin_with_coordinator_lifecycle(self):
        """æµ‹è¯•æ’ä»¶ä¸åè°ƒå™¨ç”Ÿå‘½å‘¨æœŸ"""
        context = PluginContext(
            config={'enable_monitoring': True},
            logger=logging.getLogger('test'),
            runtime={}
        )
        
        plugin = MultiModelPlugin("lifecycle_test_plugin", context)
        
        # æµ‹è¯•æ¿€æ´»
        await plugin.activate()
        assert plugin.coordinator.is_running is True
        
        # æ‰§è¡Œä¸€äº›æ“ä½œ
        plugin.register_model_tool(
            model_id="lifecycle-model",
            model_name="Lifecycle Model",
            model_type="text_generation",
            provider="test",
            capabilities=["text_generation"]
        )
        
        # æµ‹è¯•åœç”¨
        await plugin.deactivate()
        assert plugin.status.value == 'deactivated'


if __name__ == "__main__":
    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    pytest.main([__file__, "-v"])