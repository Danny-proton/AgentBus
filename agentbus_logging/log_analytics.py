"""
AgentBusæ—¥å¿—åˆ†æå·¥å…·é›†

æä¾›ä¸°å¯Œçš„æ—¥å¿—åˆ†æã€æŠ¥å‘Šç”Ÿæˆå’Œå¯è§†åŒ–å·¥å…·
"""

import os
import json
import re
import statistics
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime, timedelta
from typing import Any, Dict, List, Optional, Tuple
from pathlib import Path
from collections import defaultdict, Counter
import pandas as pd
import numpy as np
from jinja2 import Template

from .log_query import LogQuery, LogQueryEngine, LogAnalysisResult, create_query_engine, analyze_logs, create_visualizer


class LogPatternAnalyzer:
    """æ—¥å¿—æ¨¡å¼åˆ†æå™¨"""
    
    def __init__(self):
        self.patterns = {
            'timestamp': r'\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}',
            'ipv4': r'\b(?:[0-9]{1,3}\.){3}[0-9]{1,3}\b',
            'uuid': r'[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}',
            'email': r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b',
            'url': r'https?://(?:[-\w.])+(?:[:\d]+)?(?:/(?:[\w/_.])*(?:\?(?:[\w&=%.])*)?(?:#(?:\w)*)?)?',
            'json': r'\{.*\}',
            'exception': r'(Exception|Error|Traceback)',
            'stack_trace': r'File "[^"]+", line \d+, in',
            'duration': r'\d+\.?\d*\s*(ms|milliseconds?|s|seconds?|m|minutes?)',
            'file_size': r'\d+\.?\d*\s*(KB|MB|GB|TB)',
            'memory_usage': r'\d+\.?\d*\s*(MB|GB|KB)',
            'percentage': r'\d+\.?\d*%'
        }
        
    def analyze_patterns(self, messages: List[str]) -> Dict[str, Any]:
        """åˆ†ææ—¥å¿—æ¶ˆæ¯ä¸­çš„æ¨¡å¼"""
        pattern_counts = {}
        pattern_examples = {}
        
        for pattern_name, pattern_regex in self.patterns.items():
            matches = []
            for message in messages:
                found_matches = re.findall(pattern_regex, message, re.IGNORECASE)
                if found_matches:
                    matches.extend(found_matches)
                    
            if matches:
                pattern_counts[pattern_name] = len(matches)
                pattern_examples[pattern_name] = list(set(matches))[:5]  # å‰5ä¸ªå”¯ä¸€ç¤ºä¾‹
                
        return {
            "pattern_counts": pattern_counts,
            "pattern_examples": pattern_examples,
            "total_patterns_found": sum(pattern_counts.values())
        }


class ErrorAnalyzer:
    """é”™è¯¯åˆ†æå™¨"""
    
    def __init__(self):
        self.error_keywords = [
            'exception', 'error', 'failed', 'timeout', 'refused', 'denied',
            'not found', 'invalid', 'corrupt', 'abort', 'crash', 'panic'
        ]
        
    def analyze_errors(self, log_records: List) -> Dict[str, Any]:
        """åˆ†æé”™è¯¯æ—¥å¿—"""
        errors = []
        error_types = defaultdict(int)
        error_timeline = defaultdict(int)
        error_loggers = defaultdict(int)
        
        for record in log_records:
            if hasattr(record, 'level') and record.level in ['ERROR', 'CRITICAL']:
                # æ£€æŸ¥æ˜¯å¦ä¸ºé”™è¯¯æ—¥å¿—
                message_lower = record.message.lower()
                for keyword in self.error_keywords:
                    if keyword in message_lower:
                        errors.append({
                            'timestamp': record.timestamp,
                            'level': record.level,
                            'logger': record.logger,
                            'message': record.message[:200] + '...' if len(record.message) > 200 else record.message,
                            'error_type': keyword
                        })
                        error_types[keyword] += 1
                        
                        # æ—¶é—´çº¿åˆ†æ
                        try:
                            dt = datetime.fromisoformat(record.timestamp.replace('Z', '+00:00'))
                            hour = dt.hour
                            error_timeline[hour] += 1
                        except:
                            pass
                            
                        error_loggers[record.logger] += 1
                        break
                        
        return {
            "total_errors": len(errors),
            "error_types": dict(error_types),
            "error_timeline": dict(error_timeline),
            "top_error_loggers": dict(sorted(error_loggers.items(), key=lambda x: x[1], reverse=True)[:10]),
            "recent_errors": errors[-10:]  # æœ€è¿‘10ä¸ªé”™è¯¯
        }


class PerformanceAnalyzer:
    """æ€§èƒ½åˆ†æå™¨"""
    
    def analyze_performance_logs(self, log_records: List) -> Dict[str, Any]:
        """åˆ†ææ€§èƒ½æ—¥å¿—"""
        performance_logs = []
        response_times = []
        memory_usage = []
        cpu_usage = []
        
        for record in log_records:
            if hasattr(record, 'extra_fields') and record.extra_fields:
                fields = record.extra_fields
                
                # æå–å“åº”æ—¶é—´
                if 'duration' in fields:
                    try:
                        duration = float(fields['duration'])
                        response_times.append(duration)
                        performance_logs.append({
                            'timestamp': record.timestamp,
                            'type': 'response_time',
                            'value': duration,
                            'logger': record.logger
                        })
                    except ValueError:
                        pass
                        
                # æå–å†…å­˜ä½¿ç”¨
                if 'memory_usage' in fields:
                    try:
                        memory = float(fields['memory_usage'])
                        memory_usage.append(memory)
                        performance_logs.append({
                            'timestamp': record.timestamp,
                            'type': 'memory',
                            'value': memory,
                            'logger': record.logger
                        })
                    except ValueError:
                        pass
                        
                # æå–CPUä½¿ç”¨
                if 'cpu_usage' in fields:
                    try:
                        cpu = float(fields['cpu_usage'])
                        cpu_usage.append(cpu)
                        performance_logs.append({
                            'timestamp': record.timestamp,
                            'type': 'cpu',
                            'value': cpu,
                            'logger': record.logger
                        })
                    except ValueError:
                        pass
                        
        # ç»Ÿè®¡åˆ†æ
        analysis = {
            "total_performance_logs": len(performance_logs),
            "response_time_stats": self._calculate_stats(response_times, "response time"),
            "memory_stats": self._calculate_stats(memory_usage, "memory usage"),
            "cpu_stats": self._calculate_stats(cpu_usage, "cpu usage"),
            "performance_logs": performance_logs[-20:]  # æœ€è¿‘20æ¡æ€§èƒ½æ—¥å¿—
        }
        
        return analysis
        
    def _calculate_stats(self, values: List[float], name: str) -> Dict[str, Any]:
        """è®¡ç®—ç»Ÿè®¡ä¿¡æ¯"""
        if not values:
            return {f"{name}_stats": "No data available"}
            
        return {
            f"{name}_avg": statistics.mean(values),
            f"{name}_median": statistics.median(values),
            f"{name}_min": min(values),
            f"{name}_max": max(values),
            f"{name}_p95": self._percentile(values, 95),
            f"{name}_p99": self._percentile(values, 99),
            f"{name}_count": len(values)
        }
        
    def _percentile(self, data: List[float], percentile: int) -> float:
        """è®¡ç®—ç™¾åˆ†ä½æ•°"""
        if not data:
            return 0.0
        sorted_data = sorted(data)
        index = int(len(sorted_data) * percentile / 100)
        if index >= len(sorted_data):
            index = len(sorted_data) - 1
        return sorted_data[index]


class LogVisualizer:
    """æ—¥å¿—å¯è§†åŒ–å™¨"""
    
    def __init__(self, output_dir: str = "logs_analysis"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        # è®¾ç½®matplotlibä¸­æ–‡æ”¯æŒ
        plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
        plt.rcParams['axes.unicode_minus'] = False
        
    def generate_dashboard(self, analysis_result: LogAnalysisResult, 
                         error_analysis: Dict[str, Any],
                         performance_analysis: Dict[str, Any],
                         output_file: str = "dashboard.html") -> str:
        """ç”Ÿæˆåˆ†ææŠ¥å‘Šé¢æ¿"""
        
        # åˆ›å»ºHTMLæ¨¡æ¿
        template = Template("""
        <!DOCTYPE html>
        <html>
        <head>
            <title>AgentBus æ—¥å¿—åˆ†ææŠ¥å‘Š</title>
            <meta charset="UTF-8">
            <style>
                body { font-family: Arial, sans-serif; margin: 40px; }
                .header { text-align: center; color: #333; }
                .section { margin: 30px 0; padding: 20px; border: 1px solid #ddd; border-radius: 5px; }
                .metric { display: inline-block; margin: 10px; padding: 15px; background: #f5f5f5; border-radius: 5px; }
                .chart { margin: 20px 0; }
                table { width: 100%; border-collapse: collapse; margin: 20px 0; }
                th, td { border: 1px solid #ddd; padding: 12px; text-align: left; }
                th { background-color: #f2f2f2; }
                .error { color: #d9534f; }
                .warning { color: #f0ad4e; }
                .success { color: #5cb85c; }
            </style>
        </head>
        <body>
            <div class="header">
                <h1>ğŸ” AgentBus æ—¥å¿—åˆ†ææŠ¥å‘Š</h1>
                <p>ç”Ÿæˆæ—¶é—´: {{ timestamp }}</p>
            </div>
            
            <div class="section">
                <h2>ğŸ“Š åŸºæœ¬ç»Ÿè®¡</h2>
                <div class="metric">
                    <strong>æ€»æ—¥å¿—æ•°:</strong> {{ analysis.total_count }}
                </div>
                <div class="metric">
                    <strong>æ—¶é—´èŒƒå›´:</strong> {{ analysis.time_range[0] if analysis.time_range[0] else 'N/A' }} åˆ° {{ analysis.time_range[1] if analysis.time_range[1] else 'N/A' }}
                </div>
            </div>
            
            <div class="section">
                <h2>ğŸ“ˆ æ—¥å¿—çº§åˆ«åˆ†å¸ƒ</h2>
                <table>
                    <tr><th>çº§åˆ«</th><th>æ•°é‡</th><th>å æ¯”</th></tr>
                    {% for level, count in analysis.level_distribution.items() %}
                    <tr>
                        <td>{{ level }}</td>
                        <td>{{ count }}</td>
                        <td>{{ "%.1f%%"|format((count / analysis.total_count * 100) if analysis.total_count > 0 else 0) }}</td>
                    </tr>
                    {% endfor %}
                </table>
            </div>
            
            <div class="section">
                <h2>ğŸ·ï¸ Top 10 æ—¥å¿—å™¨</h2>
                <table>
                    <tr><th>æ—¥å¿—å™¨</th><th>æ—¥å¿—æ•°</th><th>å æ¯”</th></tr>
                    {% for logger, count in analysis.logger_distribution.items()[:10] %}
                    <tr>
                        <td>{{ logger }}</td>
                        <td>{{ count }}</td>
                        <td>{{ "%.1f%%"|format((count / analysis.total_count * 100) if analysis.total_count > 0 else 0) }}</td>
                    </tr>
                    {% endfor %}
                </table>
            </div>
            
            {% if analysis.error_patterns %}
            <div class="section">
                <h2>âŒ é”™è¯¯æ¨¡å¼åˆ†æ</h2>
                <table>
                    <tr><th>æ¨¡å¼</th><th>å‡ºç°æ¬¡æ•°</th><th>ç¤ºä¾‹</th></tr>
                    {% for pattern in analysis.error_patterns[:10] %}
                    <tr class="error">
                        <td>{{ pattern.pattern }}</td>
                        <td>{{ pattern.count }}</td>
                        <td>
                            {% for example in pattern.examples[:3] %}
                            <div>{{ example.timestamp }} - {{ example.message[:100] }}...</div>
                            {% endfor %}
                        </td>
                    </tr>
                    {% endfor %}
                </table>
            </div>
            {% endif %}
            
            {% if performance_analysis %}
            <div class="section">
                <h2>âš¡ æ€§èƒ½åˆ†æ</h2>
                {% if performance_analysis.response_time_stats %}
                <h3>å“åº”æ—¶é—´ç»Ÿè®¡</h3>
                <table>
                    <tr><th>æŒ‡æ ‡</th><th>å€¼</th></tr>
                    {% for key, value in performance_analysis.response_time_stats.items() %}
                    <tr><td>{{ key }}</td><td>{{ "%.3f"|format(value) if value is number else value }}</td></tr>
                    {% endfor %}
                </table>
                {% endif %}
            </div>
            {% endif %}
            
            {% if error_analysis %}
            <div class="section">
                <h2>ğŸš¨ é”™è¯¯åˆ†æ</h2>
                <div class="metric">
                    <strong>æ€»é”™è¯¯æ•°:</strong> {{ error_analysis.total_errors }}
                </div>
                <h3>é”™è¯¯ç±»å‹åˆ†å¸ƒ</h3>
                <table>
                    <tr><th>é”™è¯¯ç±»å‹</th><th>æ•°é‡</th></tr>
                    {% for error_type, count in error_analysis.error_types.items() %}
                    <tr class="error"><td>{{ error_type }}</td><td>{{ count }}</td></tr>
                    {% endfor %}
                </table>
            </div>
            {% endif %}
            
            <div class="section">
                <h2>ğŸ“‹ è¯¦ç»†åˆ†æ</h2>
                <p>æ­¤æŠ¥å‘Šç”± AgentBus å¢å¼ºæ—¥å¿—ç›‘æ§ç³»ç»Ÿè‡ªåŠ¨ç”Ÿæˆã€‚</p>
                <p>åŒ…å« {{ analysis.total_count }} æ¡æ—¥å¿—è®°å½•çš„åˆ†æç»“æœã€‚</p>
            </div>
        </body>
        </html>
        """)
        
        # ç”ŸæˆæŠ¥å‘Š
        output_path = self.output_dir / output_file
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(template.render(
                timestamp=datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                analysis=analysis_result,
                error_analysis=error_analysis,
                performance_analysis=performance_analysis
            ))
            
        return str(output_path)
        
    def create_charts(self, analysis_result: LogAnalysisResult, 
                     error_analysis: Dict[str, Any],
                     output_prefix: str = "charts") -> List[str]:
        """åˆ›å»ºåˆ†æå›¾è¡¨"""
        chart_files = []
        
        # 1. æ—¥å¿—çº§åˆ«åˆ†å¸ƒé¥¼å›¾
        if analysis_result.level_distribution:
            plt.figure(figsize=(10, 8))
            levels = list(analysis_result.level_distribution.keys())
            counts = list(analysis_result.level_distribution.values())
            
            plt.pie(counts, labels=levels, autopct='%1.1f%%', startangle=90)
            plt.title('æ—¥å¿—çº§åˆ«åˆ†å¸ƒ')
            plt.axis('equal')
            
            chart_file = self.output_dir / f"{output_prefix}_level_distribution.png"
            plt.savefig(chart_file, dpi=300, bbox_inches='tight')
            chart_files.append(str(chart_file))
            plt.close()
            
        # 2. æ—¥å¿—å™¨åˆ†å¸ƒæ¡å½¢å›¾
        if analysis_result.logger_distribution:
            plt.figure(figsize=(12, 8))
            top_loggers = dict(list(analysis_result.logger_distribution.items())[:10])
            loggers = list(top_loggers.keys())
            counts = list(top_loggers.values())
            
            plt.barh(loggers, counts)
            plt.title('Top 10 æ—¥å¿—å™¨åˆ†å¸ƒ')
            plt.xlabel('æ—¥å¿—æ•°é‡')
            plt.tight_layout()
            
            chart_file = self.output_dir / f"{output_prefix}_logger_distribution.png"
            plt.savefig(chart_file, dpi=300, bbox_inches='tight')
            chart_files.append(str(chart_file))
            plt.close()
            
        # 3. æŒ‰å°æ—¶åˆ†å¸ƒ
        if analysis_result.hourly_distribution:
            plt.figure(figsize=(12, 6))
            hours = list(analysis_result.hourly_distribution.keys())
            counts = list(analysis_result.hourly_distribution.values())
            
            plt.plot(hours, counts, marker='o', linewidth=2, markersize=6)
            plt.title('24å°æ—¶æ—¥å¿—åˆ†å¸ƒ')
            plt.xlabel('å°æ—¶')
            plt.ylabel('æ—¥å¿—æ•°é‡')
            plt.xticks(range(24))
            plt.grid(True, alpha=0.3)
            plt.tight_layout()
            
            chart_file = self.output_dir / f"{output_prefix}_hourly_distribution.png"
            plt.savefig(chart_file, dpi=300, bbox_inches='tight')
            chart_files.append(str(chart_file))
            plt.close()
            
        # 4. é”™è¯¯åˆ†æå›¾è¡¨
        if error_analysis.get('error_timeline'):
            plt.figure(figsize=(12, 6))
            hours = list(error_analysis['error_timeline'].keys())
            errors = list(error_analysis['error_timeline'].values())
            
            plt.bar(hours, errors, color='red', alpha=0.7)
            plt.title('24å°æ—¶é”™è¯¯åˆ†å¸ƒ')
            plt.xlabel('å°æ—¶')
            plt.ylabel('é”™è¯¯æ•°é‡')
            plt.xticks(range(24))
            plt.grid(True, alpha=0.3)
            plt.tight_layout()
            
            chart_file = self.output_dir / f"{output_prefix}_error_timeline.png"
            plt.savefig(chart_file, dpi=300, bbox_inches='tight')
            chart_files.append(str(chart_file))
            plt.close()
            
        return chart_files


class AnomalyDetector:
    """å¼‚å¸¸æ£€æµ‹å™¨"""
    
    def __init__(self, window_size: int = 100, threshold: float = 2.0):
        self.window_size = window_size
        self.threshold = threshold
        
    def detect_log_volume_anomalies(self, hourly_distribution: Dict[int, int]) -> List[Dict[str, Any]]:
        """æ£€æµ‹æ—¥å¿—é‡å¼‚å¸¸"""
        hours = sorted(hourly_distribution.keys())
        volumes = [hourly_distribution[h] for h in hours]
        
        anomalies = []
        
        # ä½¿ç”¨æ»‘åŠ¨çª—å£æ£€æµ‹å¼‚å¸¸
        for i in range(self.window_size, len(volumes)):
            window = volumes[i-self.window_size:i]
            current_volume = volumes[i]
            
            # è®¡ç®—çª—å£ç»Ÿè®¡ä¿¡æ¯
            mean_volume = statistics.mean(window)
            std_volume = statistics.stdev(window) if len(window) > 1 else 0
            
            # æ£€æµ‹å¼‚å¸¸
            if std_volume > 0:
                z_score = abs(current_volume - mean_volume) / std_volume
                if z_score > self.threshold:
                    anomalies.append({
                        "hour": hours[i],
                        "volume": current_volume,
                        "expected": mean_volume,
                        "z_score": z_score,
                        "type": "high" if current_volume > mean_volume else "low"
                    })
                    
        return anomalies
        
    def detect_error_rate_anomalies(self, error_data: Dict[int, int]) -> List[Dict[str, Any]]:
        """æ£€æµ‹é”™è¯¯ç‡å¼‚å¸¸"""
        return self.detect_log_volume_anomalies(error_data)
        
    def detect_performance_anomalies(self, performance_data: List[float]) -> List[Dict[str, int]]:
        """æ£€æµ‹æ€§èƒ½å¼‚å¸¸"""
        if len(performance_data) < self.window_size:
            return []
            
        anomalies = []
        
        for i in range(self.window_size, len(performance_data)):
            window = performance_data[i-self.window_size:i]
            current_value = performance_data[i]
            
            mean_val = statistics.mean(window)
            std_val = statistics.stdev(window) if len(window) > 1 else 0
            
            if std_val > 0:
                z_score = abs(current_value - mean_val) / std_val
                if z_score > self.threshold:
                    anomalies.append({
                        "index": i,
                        "value": current_value,
                        "expected": mean_val,
                        "z_score": z_score,
                        "type": "high" if current_value > mean_val else "low"
                    })
                    
        return anomalies


class LogReporter:
    """æ—¥å¿—æŠ¥å‘Šç”Ÿæˆå™¨"""
    
    def __init__(self, query_engine: LogQueryEngine):
        self.query_engine = query_engine
        self.pattern_analyzer = LogPatternAnalyzer()
        self.error_analyzer = ErrorAnalyzer()
        self.performance_analyzer = PerformanceAnalyzer()
        self.anomaly_detector = AnomalyDetector()
        self.visualizer = LogVisualizer()
        
    def generate_comprehensive_report(self, 
                                    start_time: datetime,
                                    end_time: datetime,
                                    output_dir: str = "reports") -> Dict[str, str]:
        """ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š"""
        
        # 1. æŸ¥è¯¢æ—¥å¿—æ•°æ®
        query = LogQuery(
            start_time=start_time,
            end_time=end_time,
            limit=10000  # é™åˆ¶æŸ¥è¯¢æ•°é‡
        )
        
        log_records = self.query_engine.query(query)
        
        if not log_records:
            return {"error": "æ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„æ—¥å¿—è®°å½•"}
            
        # 2. æ‰§è¡Œå„é¡¹åˆ†æ
        print("æ­£åœ¨æ‰§è¡Œæ—¥å¿—åˆ†æ...")
        analysis_result = analyze_logs(log_records)
        print("åŸºæœ¬åˆ†æå®Œæˆ")
        
        print("æ­£åœ¨åˆ†æé”™è¯¯æ¨¡å¼...")
        error_analysis = self.error_analyzer.analyze_errors(log_records)
        print("é”™è¯¯åˆ†æå®Œæˆ")
        
        print("æ­£åœ¨åˆ†ææ€§èƒ½æ•°æ®...")
        performance_analysis = self.performance_analyzer.analyze_performance_logs(log_records)
        print("æ€§èƒ½åˆ†æå®Œæˆ")
        
        # 3. å¼‚å¸¸æ£€æµ‹
        print("æ­£åœ¨æ£€æµ‹å¼‚å¸¸...")
        log_anomalies = self.anomaly_detector.detect_log_volume_anomalies(
            analysis_result.hourly_distribution
        )
        error_anomalies = self.anomaly_detector.detect_error_rate_anomalies(
            error_analysis.get('error_timeline', {})
        )
        print("å¼‚å¸¸æ£€æµ‹å®Œæˆ")
        
        # 4. ç”ŸæˆæŠ¥å‘Š
        print("æ­£åœ¨ç”ŸæˆæŠ¥å‘Š...")
        
        # ç”Ÿæˆè¯¦ç»†JSONæŠ¥å‘Š
        json_report = {
            "metadata": {
                "report_type": "comprehensive",
                "generated_at": datetime.utcnow().isoformat(),
                "time_range": {
                    "start": start_time.isoformat(),
                    "end": end_time.isoformat()
                },
                "total_records": len(log_records)
            },
            "analysis": asdict(analysis_result),
            "error_analysis": error_analysis,
            "performance_analysis": performance_analysis,
            "anomalies": {
                "log_volume": log_anomalies,
                "error_rate": error_anomalies
            }
        }
        
        # ä¿å­˜JSONæŠ¥å‘Š
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)
        
        json_file = output_path / f"report_{start_time.strftime('%Y%m%d_%H%M%S')}.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            import json
            json.dump(json_report, f, indent=2, ensure_ascii=False)
            
        # ç”ŸæˆHTMLæŠ¥å‘Š
        html_file = self.visualizer.generate_dashboard(
            analysis_result,
            error_analysis,
            performance_analysis,
            f"dashboard_{start_time.strftime('%Y%m%d_%H%M%S')}.html"
        )
        
        # ç”Ÿæˆå›¾è¡¨
        chart_files = self.visualizer.create_charts(
            analysis_result,
            error_analysis,
            f"charts_{start_time.strftime('%Y%m%d_%H%M%S')}"
        )
        
        return {
            "json_report": str(json_file),
            "html_report": html_file,
            "chart_files": chart_files,
            "summary": {
                "total_records": len(log_records),
                "total_errors": error_analysis.get('total_errors', 0),
                "anomalies_detected": len(log_anomalies) + len(error_anomalies),
                "report_files": [str(json_file), html_file] + chart_files
            }
        }


# ä¾¿æ·å‡½æ•°
def quick_log_analysis(log_dirs: List[str], start_time: datetime, end_time: datetime,
                     output_dir: str = "quick_analysis") -> Dict[str, Any]:
    """å¿«é€Ÿæ—¥å¿—åˆ†æ"""
    query_engine = create_query_engine(log_dirs, "/tmp/logs/index")
    reporter = LogReporter(query_engine)
    return reporter.generate_comprehensive_report(start_time, end_time, output_dir)


def detect_log_anomalies(log_records: List) -> Dict[str, Any]:
    """æ£€æµ‹æ—¥å¿—å¼‚å¸¸"""
    analyzer = AnomalyDetector()
    
    # æ¨¡æ‹Ÿå°æ—¶åˆ†å¸ƒæ•°æ®
    hourly_dist = defaultdict(int)
    for record in log_records:
        try:
            dt = datetime.fromisoformat(record.timestamp.replace('Z', '+00:00'))
            hourly_dist[dt.hour] += 1
        except:
            pass
            
    # æ¨¡æ‹Ÿé”™è¯¯æ•°æ®
    error_dist = defaultdict(int)
    for record in log_records:
        if hasattr(record, 'level') and record.level in ['ERROR', 'CRITICAL']:
            try:
                dt = datetime.fromisoformat(record.timestamp.replace('Z', '+00:00'))
                error_dist[dt.hour] += 1
            except:
                pass
    
    return {
        "log_volume_anomalies": analyzer.detect_log_volume_anomalies(dict(hourly_dist)),
        "error_rate_anomalies": analyzer.detect_error_rate_anomalies(dict(error_dist))
    }