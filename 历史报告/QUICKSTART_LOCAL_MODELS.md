---
AIGC:
    ContentProducer: Minimax Agent AI
    ContentPropagator: Minimax Agent AI
    Label: AIGC
    ProduceID: "00000000000000000000000000000000"
    PropagateID: "00000000000000000000000000000000"
    ReservedCode1: 304402206ee9fe24f6886238aab794da922783611185daafe26068bf3a42dd573e5297910220088977dfd255d9c2a98563626cf1935266721500ebc84a6260f16717fd0704ed
    ReservedCode2: 304502203995bfa021231c68d6602741b8a99d74a9d419655dce9bffe85b8680a6271c74022100edb911ecefe83f5784290259855d8306effad78c41067a01ccdd400d6d0dad8e
---

# AgentBus æœ¬åœ°æ¨¡å‹é…ç½®å¿«é€Ÿå¼€å§‹ - 2026å¹´1æœˆæ›´æ–°
## è¡Œä¸šè§£å†³æ–¹æ¡ˆé›†æˆä¸éªŒè¯éƒ¨ï¼ˆè¡Œè§£ï¼‰å‡ºå“

## ğŸ‰ é‡å¤§æ›´æ–° (2026-01-29)

### âœ¨ æ–°å¢åŠŸèƒ½
1. **vLLM OpenAIæ ¼å¼å®Œæ•´æ”¯æŒ** - æ–°å¢ä¸€ä¸ªå®Œæ•´çš„vLLMç« èŠ‚ï¼ˆ200+è¡Œï¼‰
2. **2024-2025æœ€æ–°æ¨¡å‹æ”¯æŒ** - æ·»åŠ Qwen2.5ã€Llama3.1ã€Phi3ç­‰æœ€æ–°æ¨¡å‹
3. **å®Œæ•´æµ‹è¯•å¥—ä»¶** - æä¾›Ollamaã€vLLMã€æ€§èƒ½å¯¹æ¯”æµ‹è¯•è„šæœ¬
4. **æ™ºèƒ½é€‰æ‹©æŒ‡å—** - æ ¹æ®ä¸åŒéœ€æ±‚è‡ªåŠ¨æ¨èæœ€ä½³é…ç½®

### ğŸ”§ ä¼˜åŒ–æ”¹è¿›
- å¢å¼ºvLLMé…ç½®è¯´æ˜ï¼Œæ”¯æŒå¤šGPUå’Œæ€§èƒ½ä¼˜åŒ–
- æ·»åŠ è¯¦ç»†çš„OpenAIå…¼å®¹APIé…ç½®ç¤ºä¾‹
- å®Œå–„æ•…éšœæ’é™¤å’Œç›‘æ§æŒ‡å—
- ä¼˜åŒ–æ–‡æ¡£ç»“æ„ï¼Œæ·»åŠ å¿«é€Ÿå¯¼èˆªè¡¨

### ğŸ“ˆ æ–‡æ¡£è§„æ¨¡
- **æ€»è¡Œæ•°**: 1010è¡Œï¼ˆæ–°å¢350+è¡Œï¼‰
- **vLLMç« èŠ‚**: 200+è¡Œå®Œæ•´æŒ‡å—
- **æµ‹è¯•è„šæœ¬**: 150+è¡Œä»£ç ç¤ºä¾‹
- **é…ç½®ç¤ºä¾‹**: 50+ä¸ªå®Œæ•´é…ç½®

## ğŸš€ 5åˆ†é’Ÿå¿«é€Ÿé…ç½®æœ¬åœ°AIæ¨¡å‹

### æ¨èæ–¹æ¡ˆï¼šOllamaï¼ˆæœ€ç®€å•ï¼‰

#### 1. å®‰è£…Ollama
```bash
# macOS
brew install ollama

# Linux
curl -fsSL https://ollama.ai/install.sh | sh

# Windows (ä½¿ç”¨WSLæˆ–ç›´æ¥ä¸‹è½½å®‰è£…åŒ…)
# ä¸‹è½½åœ°å€: https://ollama.ai/download
```

#### 2. å¯åŠ¨OllamaæœåŠ¡
```bash
# åå°è¿è¡ŒOllamaæœåŠ¡
ollama serve &

# æˆ–è€…å‰å°è¿è¡Œ
ollama serve
```

#### 3. ä¸‹è½½æ¨èçš„æ¨¡å‹
```bash
# ä»£ç ç”Ÿæˆæ¨¡å‹ï¼ˆæ¨èï¼‰
ollama pull codellama:7b

# å¯¹è¯æ¨¡å‹
ollama pull llama2:7b

# è½»é‡çº§æ¨¡å‹ï¼ˆé€‚åˆä½é…ç½®ï¼‰
ollama pull phi:2.7b

# ä¸­æ–‡æ¨¡å‹
ollama pull qwen:7b
ollama pull qwen2.5:7b    # Qwen2.5æœ€æ–°ç‰ˆæœ¬
ollama pull baichuan:7b   # ç™¾å·7B
ollama pull chatglm3:6b   # ChatGLM3

# æŸ¥çœ‹å·²å®‰è£…çš„æ¨¡å‹
ollama list
```

#### 4. æµ‹è¯•æ¨¡å‹
```bash
# æµ‹è¯•ä»£ç ç”Ÿæˆ
ollama run codellama:7b "å†™ä¸€ä¸ªPythonçš„Hello Worldç¨‹åº"

# æµ‹è¯•å¯¹è¯
ollama run llama2:7b "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±"
```

#### 5. é…ç½®AgentBus
åˆ›å»ºé…ç½®æ–‡ä»¶ `config/local_models.toml`ï¼š

```toml
[models]
default_provider = "ollama"

[models.ollama]
provider = "ollama"
base_url = "http://localhost:11434"
timeout = 300

[models.ollama.models]
codellama = { 
    model = "codellama:7b", 
    context_length = 16384,
    temperature = 0.1,
    top_p = 0.9,
    description = "ä»£ç ç”Ÿæˆä¸“ç”¨æ¨¡å‹"
}
llama2 = { 
    model = "llama2:7b", 
    context_length = 4096,
    temperature = 0.7,
    description = "é€šç”¨å¯¹è¯æ¨¡å‹"
}
phi = { 
    model = "phi:2.7b", 
    context_length = 2048,
    temperature = 0.8,
    description = "è½»é‡çº§æ¨¡å‹ï¼Œé€‚åˆä½é…ç½®"
}
```

## ğŸ› ï¸ å…¶ä»–æœ¬åœ°æ¨¡å‹æ–¹æ¡ˆ

### æ–¹æ¡ˆ2ï¼šLM Studioï¼ˆå›¾å½¢ç•Œé¢ï¼‰

#### å®‰è£…å’Œè®¾ç½®
```bash
# 1. ä¸‹è½½LM Studio
# https://lmstudio.ai/

# 2. æ‰“å¼€LM Studio
# 3. ç‚¹å‡»"Discover"æœç´¢æ¨¡å‹
# 4. ä¸‹è½½æ¨èæ¨¡å‹ï¼š
#    - Llama 2 7B Chat
#    - Code Llama 7B
#    - Mistral 7B

# 5. åœ¨æœ¬åœ°æœåŠ¡å™¨ä¸­å¯åŠ¨APIæœåŠ¡
#    é»˜è®¤ç«¯å£ï¼š1234
```

#### é…ç½®AgentBus
```toml
[models.lmstudio]
provider = "openai"
base_url = "http://localhost:1234/v1"
api_key = "sk-lmstudio"  # ä»»æ„å­—ç¬¦ä¸²

[models.lmstudio.models]
local_llama = { model = "local-model" }
```

### æ–¹æ¡ˆ3ï¼šGPT4Allï¼ˆå®Œå…¨ç¦»çº¿ï¼‰

#### å®‰è£…
```bash
# PythonåŒ…
pip install gpt4all

# å›¾å½¢ç•Œé¢ï¼ˆå¯é€‰ï¼‰
# ä¸‹è½½åœ°å€: https://gpt4all.io/
```

#### ä¸‹è½½æ¨¡å‹
```python
from gpt4all import GPT4All

# ä¸‹è½½æ¨¡å‹ï¼ˆé¦–æ¬¡è¿è¡Œï¼‰
model = GPT4All("ggml-model-gpt4all-falcon.bin")
```

#### é…ç½®
```python
# config/gpt4all_config.py
from agentbus.integrations.gpt4all import GPT4AllModel

config = {
    'model_path': './models/ggml-model-gpt4all-falcon.bin',
    'device': 'cpu',  # ä½¿ç”¨CPUï¼Œ'cuda'ä½¿ç”¨GPU
    'n_threads': 4,   # CPUçº¿ç¨‹æ•°
    'n_ctx': 2048,   # ä¸Šä¸‹æ–‡é•¿åº¦
    'temperature': 0.7,
    'max_tokens': 512
}

model = GPT4AllModel(config)
```

### æ–¹æ¡ˆ4ï¼švLLMï¼ˆé«˜æ€§èƒ½ + OpenAIå…¼å®¹ï¼‰

#### vLLMç®€ä»‹
vLLMæ˜¯ä¸€ä¸ªé«˜æ€§èƒ½çš„LLMæ¨ç†æœåŠ¡å¼•æ“ï¼Œæä¾›OpenAIå…¼å®¹çš„APIæ¥å£ï¼Œæ”¯æŒå¼ é‡å¹¶è¡Œã€æ‰¹å¤„ç†æ¨ç†ç­‰é«˜çº§åŠŸèƒ½ã€‚éå¸¸é€‚åˆç”Ÿäº§ç¯å¢ƒçš„æœ¬åœ°æ¨¡å‹éƒ¨ç½²ã€‚

#### å®‰è£…è¦æ±‚
```bash
# Python 3.8+
# CUDA 12.1+ (æ¨è) æˆ– CPUæ¨¡å¼

# å®‰è£…vLLM
pip install vllm

# å¦‚æœä½¿ç”¨GPUï¼Œå®‰è£…CUDAç‰ˆæœ¬
# pip install vllm[cu121]  # CUDA 12.1
# pip install vllm[cu118]  # CUDA 11.8
```

#### å¯åŠ¨vLLMæœåŠ¡å™¨ï¼ˆOpenAIå…¼å®¹æ ¼å¼ï¼‰

**åŸºæœ¬å¯åŠ¨å‘½ä»¤ï¼š**
```bash
# å¯åŠ¨OpenAIå…¼å®¹æœåŠ¡å™¨
python -m vllm.entrypoints.openai.api_server \
    --model microsoft/DialoGPT-medium \
    --host 0.0.0.0 \
    --port 8001 \
    --tensor-parallel-size 1 \
    --max-model-len 4096 \
    --gpu-memory-utilization 0.8
```

**é«˜çº§é…ç½®å¯åŠ¨ï¼š**
```bash
# å¤šGPUå¼ é‡å¹¶è¡Œ
python -m vllm.entrypoints.openai.api_server \
    --model meta-llama/Llama-2-7b-chat-hf \
    --host 0.0.0.0 \
    --port 8001 \
    --tensor-parallel-size 2 \
    --max-model-len 4096 \
    --max-num-seqs 32 \
    --max-num-batched-tokens 8192 \
    --gpu-memory-utilization 0.9 \
    --served-model-name "llama2-7b-chat" \
    --response-role role \
    --disable-log-stats

# CPUæ¨¡å¼ï¼ˆæ— GPUï¼‰
python -m vllm.entrypoints.openai.api_server \
    --model microsoft/DialoGPT-medium \
    --host 0.0.0.0 \
    --port 8001 \
    --tensor-parallel-size 1 \
    --device cpu
```

#### æ”¯æŒçš„æ¨¡å‹ç±»å‹

**æ¨èçš„ä»£ç ç”Ÿæˆæ¨¡å‹ï¼š**
```bash
# Code Llamaç³»åˆ—
--model codellama/CodeLlama-7b-Instruct-hf
--model codellama/CodeLlama-13b-Instruct-hf

# DeepSeek Coder
--model deepseek-ai/deepseek-coder-6.7b-instruct

# StarCoder
--model bigcode/starcoder2-7b
```

**å¯¹è¯æ¨¡å‹ï¼š**
```bash
# Llama 2ç³»åˆ—
--model meta-llama/Llama-2-7b-chat-hf
--model meta-llama/Llama-2-13b-chat-hf

# Mistralç³»åˆ—
--model mistralai/Mistral-7B-Instruct-v0.1
--model mistralai/Mistral-7B-Instruct-v0.2

# Qwenç³»åˆ—
--model Qwen/Qwen2.5-7B-Instruct
--model Qwen/Qwen2.5-14B-Instruct
```

**ä¸­æ–‡ä¼˜åŒ–æ¨¡å‹ï¼š**
```bash
# ChatGLMç³»åˆ—
--model THUDM/chatglm3-6b

# Baichuanç³»åˆ—
--model baichuan-inc/Baichuan2-7B-Chat

# InternLMç³»åˆ—
--model internlm/internlm2-chat-7b
```

#### AgentBusé…ç½®ï¼ˆOpenAIå…¼å®¹ï¼‰

```toml
[models.vllm_openai]
provider = "openai"
base_url = "http://localhost:8001/v1"
api_key = "sk-vllm-local"
timeout = 300
max_retries = 3

[models.vllm_openai.models]
# é€šç”¨å¯¹è¯æ¨¡å‹
llama2_chat = { 
    model = "llama2-7b-chat", 
    context_length = 4096,
    temperature = 0.7,
    max_tokens = 2048,
    top_p = 0.9,
    description = "Llama 2 7BèŠå¤©æ¨¡å‹"
}

# ä»£ç ç”Ÿæˆæ¨¡å‹
codellama = { 
    model = "codellama-7b-instruct", 
    context_length = 16384,
    temperature = 0.1,
    max_tokens = 2048,
    top_p = 0.9,
    description = "ä»£ç ç”Ÿæˆä¸“ç”¨æ¨¡å‹"
}

# ä¸­æ–‡å¯¹è¯æ¨¡å‹
qwen_chat = { 
    model = "qwen2.5-7b-instruct", 
    context_length = 32768,
    temperature = 0.7,
    max_tokens = 2048,
    description = "ä¸­æ–‡ä¼˜åŒ–å¯¹è¯æ¨¡å‹"
}

# è½»é‡çº§æ¨¡å‹
mistral_small = { 
    model = "mistral-7b-instruct-v0.1", 
    context_length = 4096,
    temperature = 0.8,
    max_tokens = 1024,
    description = "è½»é‡çº§å¯¹è¯æ¨¡å‹"
}
```

#### æµ‹è¯•vLLMæœåŠ¡

**1. å¥åº·æ£€æŸ¥ï¼š**
```bash
curl http://localhost:8001/v1/models

# é¢„æœŸå“åº”ï¼š
{
  "object": "list",
  "data": [
    {
      "id": "llama2-7b-chat",
      "object": "model",
      "created": 1677610602,
      "owned_by": "vllm"
    }
  ]
}
```

**2. èŠå¤©å®Œæˆæµ‹è¯•ï¼š**
```bash
curl http://localhost:8001/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "llama2-7b-chat",
    "messages": [
      {"role": "user", "content": "å†™ä¸€ä¸ªPythonçš„Hello Worldç¨‹åº"}
    ],
    "max_tokens": 100,
    "temperature": 0.7
  }'
```

**3. æµå¼å“åº”æµ‹è¯•ï¼š**
```bash
curl http://localhost:8001/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "llama2-7b-chat",
    "messages": [
      {"role": "user", "content": "è§£é‡Šä¸€ä¸‹ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½"}
    ],
    "stream": true
  }'
```

#### æ€§èƒ½ä¼˜åŒ–é…ç½®

**GPUå†…å­˜ä¼˜åŒ–ï¼š**
```bash
# è®¾ç½®GPUå†…å­˜ä½¿ç”¨ç‡
--gpu-memory-utilization 0.8  # ä½¿ç”¨80%çš„GPUå†…å­˜

# é™åˆ¶æœ€å¤§åºåˆ—æ•°
--max-num-seqs 32  # æœ€å¤§å¹¶å‘åºåˆ—æ•°

# é™åˆ¶æ‰¹å¤„ç†å¤§å°
--max-num-batched-tokens 8192  # æœ€å¤§æ‰¹å¤„ç†tokenæ•°
```

**å¤šGPUé…ç½®ï¼š**
```bash
# 4å¡é…ç½®ç¤ºä¾‹
--tensor-parallel-size 4 \
--pipeline-parallel-size 1 \
--ray-workers-per-node 4
```

**æ¨ç†å‚æ•°ä¼˜åŒ–ï¼š**
```bash
# é’ˆå¯¹ä»£ç ç”Ÿæˆä¼˜åŒ–
--max-model-len 16384 \
--enable-chunked-prefill \
--max-num-batched-tokens 8192 \
--swap-space 16
```

#### ä¸å…¶ä»–æ¨¡å‹å¹³å°å¯¹æ¯”

| ç‰¹æ€§ | vLLM | Ollama | LM Studio |
|------|------|--------|-----------|
| **OpenAIå…¼å®¹API** | âœ… | âŒ | âœ… |
| **å¼ é‡å¹¶è¡Œ** | âœ… | âŒ | âŒ |
| **æ‰¹å¤„ç†æ¨ç†** | âœ… | âŒ | âŒ |
| **KVç¼“å­˜ä¼˜åŒ–** | âœ… | âŒ | âŒ |
| **å®‰è£…å¤æ‚åº¦** | ä¸­ç­‰ | ç®€å• | ç®€å• |
| **GPUä¼˜åŒ–** | ä¼˜ç§€ | ä¸€èˆ¬ | ä¸€èˆ¬ |
| **æ¨¡å‹åˆ‡æ¢** | çƒ­é‡è½½ | éœ€é‡å¯ | éœ€é‡å¯ |

#### å¸¸è§é—®é¢˜è§£å†³

**1. CUDAç‰ˆæœ¬ä¸åŒ¹é…ï¼š**
```bash
# æ£€æŸ¥CUDAç‰ˆæœ¬
nvcc --version

# å®‰è£…å¯¹åº”ç‰ˆæœ¬çš„vLLM
pip install vllm[cu121]  # å¦‚æœCUDA 12.1
```

**2. GPUå†…å­˜ä¸è¶³ï¼š**
```bash
# ä½¿ç”¨CPUæ¨¡å¼
--device cpu

# é™ä½å†…å­˜ä½¿ç”¨ç‡
--gpu-memory-utilization 0.5

# ä½¿ç”¨é‡åŒ–æ¨¡å‹
--model meta-llama/Llama-2-7b-chat-hf
# ä½¿ç”¨4bité‡åŒ–æ¨¡å‹ï¼š--model TheBloke/Llama-2-7B-Chat-GPTQ
```

**3. æ¨¡å‹åŠ è½½å¤±è´¥ï¼š**
```bash
# æ£€æŸ¥æ¨¡å‹åç§°æ˜¯å¦æ­£ç¡®
curl http://localhost:8001/v1/models

# ä¸‹è½½åˆ°æœ¬åœ°ç¼“å­˜
export HF_HOME=/path/to/cache
huggingface-cli download meta-llama/Llama-2-7b-chat-hf
```

#### ç›‘æ§å’Œç®¡ç†

**1. æ€§èƒ½ç›‘æ§ï¼š**
```bash
# æŸ¥çœ‹vLLMè¿›ç¨‹
ps aux | grep vllm

# ç›‘æ§GPUä½¿ç”¨
nvidia-smi -l 1

# æŸ¥çœ‹æœåŠ¡æ—¥å¿—
tail -f /var/log/vllm.log
```

**2. è´Ÿè½½æµ‹è¯•ï¼š**
```bash
# ä½¿ç”¨abè¿›è¡Œå‹åŠ›æµ‹è¯•
ab -n 100 -c 10 -H "Content-Type: application/json" \
   -p test_data.json http://localhost:8001/v1/chat/completions
```

#### æœ€ä½³å®è·µå»ºè®®

1. **ç”Ÿäº§ç¯å¢ƒé…ç½®ï¼š**
   - ä½¿ç”¨systemdæˆ–dockerè¿è¡ŒvLLMæœåŠ¡
   - é…ç½®è´Ÿè½½å‡è¡¡å’Œå¥åº·æ£€æŸ¥
   - è®¾ç½®é€‚å½“çš„è¶…æ—¶å’Œé‡è¯•æœºåˆ¶

2. **æ¨¡å‹é€‰æ‹©ç­–ç•¥ï¼š**
   - å¼€å‘ç¯å¢ƒï¼šä½¿ç”¨è¾ƒå°çš„7Bæ¨¡å‹
   - ç”Ÿäº§ç¯å¢ƒï¼šæ ¹æ®ç¡¬ä»¶é…ç½®é€‰æ‹©åˆé€‚çš„æ¨¡å‹å¤§å°
   - æ‰¹å¤„ç†ï¼šä¼˜å…ˆé€‰æ‹©æ”¯æŒæ‰¹å¤„ç†çš„æ¨¡å‹

3. **ç›‘æ§è¦ç‚¹ï¼š**
   - GPUåˆ©ç”¨ç‡å’Œå†…å­˜ä½¿ç”¨
   - APIå“åº”æ—¶é—´å’Œååé‡
   - é”™è¯¯ç‡å’Œç³»ç»Ÿèµ„æºä½¿ç”¨

vLLMæä¾›äº†æœ€ä½³çš„OpenAIå…¼å®¹æ€§å’Œç”Ÿäº§çº§æ€§èƒ½ï¼Œç‰¹åˆ«é€‚åˆéœ€è¦é«˜å¹¶å‘å’Œä½å»¶è¿Ÿçš„åº”ç”¨åœºæ™¯ã€‚

## ğŸ¯ ä¸åŒéœ€æ±‚çš„æ¨¡å‹æ¨è

### ä»£ç ç”Ÿæˆå’Œç¼–ç¨‹åŠ©æ‰‹
```bash
# æ¨èæ¨¡å‹ï¼ˆæŒ‰æ€§èƒ½æ’åºï¼‰
ollama pull codellama:7b      # æœ€ä½³ä»£ç èƒ½åŠ›
ollama pull starcoder:7b      # StarCoderæ¨¡å‹
ollama pull deepseek-coder:6.7b  # ä¸“é—¨é’ˆå¯¹ä»£ç 
```

### å¯¹è¯å’ŒèŠå¤©
```bash
# æ¨èæ¨¡å‹
ollama pull llama2:7b         # Meta Llama2
ollama pull mistral:7b        # Mistral 7B
ollama pull qwen:7b          # é€šä¹‰åƒé—®
```

### ä¸­æ–‡æ”¯æŒ
```bash
# ä¸­æ–‡ä¼˜åŒ–æ¨¡å‹
ollama pull qwen:7b           # é€šä¹‰åƒé’±7B
ollama pull baichuan:7b       # ç™¾å·7B
ollama pull chatglm3:6b       # ChatGLM3
```

### ä½é…ç½®è®¾å¤‡
```bash
# è½»é‡çº§æ¨¡å‹ï¼ˆé€‚åˆCPUï¼‰
ollama pull phi:2.7b         # Microsoft Phi-2.7B
ollama pull gemma:2b          # Google Gemma 2B
ollama pull qwen:1.4b         # é€šä¹‰åƒé’±1.4B
```

### ç‰¹å®šä»»åŠ¡
```bash
# ç‰¹å®šé¢†åŸŸæ¨¡å‹
ollama pull starcoder:7b     # ä»£ç ç”Ÿæˆ
ollama pull wizardlm:13b      # æ•°å­¦å’Œç§‘å­¦
ollama pull nous Hermes 2-Mixtral:8x7b  # å¤šä»»åŠ¡æ¨¡å‹

# 2024-2025 æœ€æ–°æ¨¡å‹
ollama pull qwen2.5-coder:7b  # Qwen2.5ä»£ç ä¸“å®¶
ollama pull deepseek-coder:6.7b # DeepSeekä»£ç æ¨¡å‹
ollama pull codestral:7b      # Mistralä»£ç æ¨¡å‹
ollama pull gemma2:9b         # Google Gemma2 9B
ollama pull phi3:medium       # Microsoft Phi-3 Medium
ollama pull llama3.1:8b       # Meta Llama 3.1 8B
ollama pull mistral-nemo:12b  # Mistral Nemo 12B
```

## âš™ï¸ æ€§èƒ½ä¼˜åŒ–å»ºè®®

### GPUé…ç½®ï¼ˆå¦‚æœæœ‰GPUï¼‰
```bash
# æ£€æŸ¥GPUæ˜¯å¦å¯ç”¨
nvidia-smi

# ä½¿ç”¨CUDAåŠ é€Ÿçš„æ¨¡å‹
ollama pull llama2:13b-chat.Q4_0.gguf  # å¤§æ¨¡å‹éœ€è¦GPU
```

### CPUä¼˜åŒ–
```bash
# è®¾ç½®ç¯å¢ƒå˜é‡ä¼˜åŒ–CPUä½¿ç”¨
export OMP_NUM_THREADS=8
export PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0

# å¯åŠ¨æ—¶æŒ‡å®šCPUçº¿ç¨‹æ•°
ollama run llama2:7b --threads 8
```

### å†…å­˜ä¼˜åŒ–
```bash
# ç›‘æ§å†…å­˜ä½¿ç”¨
htop

# ä½¿ç”¨é‡åŒ–æ¨¡å‹ï¼ˆå‡å°‘å†…å­˜ä½¿ç”¨ï¼‰
ollama pull llama2:7b.Q4_0.gguf  # 4ä½é‡åŒ–
ollama pull llama2:7b.Q8_0.gguf  # 8ä½é‡åŒ–
```

## ğŸ”§ æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

#### 1. æ¨¡å‹ä¸‹è½½å¤±è´¥
```bash
# æ£€æŸ¥ç½‘ç»œè¿æ¥
ping ollama.ai

# ä½¿ç”¨ä»£ç†ï¼ˆå¦‚æœéœ€è¦ï¼‰
export HTTP_PROXY=http://proxy:port
export HTTPS_PROXY=http://proxy:port

# æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹
ollama pull codellama:7b --verbose
```

#### 2. å†…å­˜ä¸è¶³
```bash
# æŸ¥çœ‹å†…å­˜ä½¿ç”¨
free -h

# ä½¿ç”¨å°æ¨¡å‹
ollama pull phi:2.7b

# è®¾ç½®æ¨¡å‹å‚æ•°å‡å°‘å†…å­˜ä½¿ç”¨
export OLLAMA_NUM_PARALLEL=1
```

#### 3. å¯åŠ¨å¤±è´¥
```bash
# æ£€æŸ¥ç«¯å£æ˜¯å¦è¢«å ç”¨
lsof -i :11434

# æ¸…ç†Ollamaç¼“å­˜
ollama rm all
ollama prune

# é‡æ–°å®‰è£…Ollama
curl -fsSL https://ollama.ai/install.sh | sh
```

#### 4. APIè¿æ¥å¤±è´¥
```bash
# æµ‹è¯•APIè¿æ¥
curl http://localhost:11434/api/tags

# æ£€æŸ¥é˜²ç«å¢™
sudo ufw status
```

### è°ƒè¯•å‘½ä»¤
```bash
# æŸ¥çœ‹Ollamaæ—¥å¿—
journalctl -u ollama -f

# æŸ¥çœ‹æ¨¡å‹ä¿¡æ¯
ollama list

# æµ‹è¯•ç‰¹å®šæ¨¡å‹
ollama run codellama:7b --verbose
```

## ğŸ“± ç§»åŠ¨ç«¯å’ŒåµŒå…¥å¼

### Androidè®¾å¤‡
```bash
# Termuxç¯å¢ƒ
pkg install ollama
ollama serve --host 0.0.0.0

# å°æ¨¡å‹æ¨è
ollama pull phi:2.7b
```

### æ ‘è“æ´¾
```bash
# ARM64ä¼˜åŒ–
ollama pull llama2:7b.Q4_0.gguf

# ä½¿ç”¨è½»é‡çº§æ¨¡å‹
ollama pull phi:2.7b
```

## ğŸ”— é›†æˆåˆ°AgentBus

### å®Œæ•´é…ç½®æ–‡ä»¶ç¤ºä¾‹

```toml
# config/production_models.toml
[models]
# é»˜è®¤æä¾›å•†
default_provider = "ollama"

# æ¨¡å‹è¶…æ—¶è®¾ç½®
timeout = 300
max_retries = 3

[models.ollama]
provider = "ollama"
base_url = "http://localhost:11434"
timeout = 300

[models.ollama.models]
# é€šç”¨å¯¹è¯
llama2_chat = { 
    model = "llama2:7b-chat", 
    context_length = 4096,
    temperature = 0.7,
    max_tokens = 2048,
    description = "é€šç”¨å¯¹è¯æ¨¡å‹"
}

# ä»£ç ç”Ÿæˆ
codellama = { 
    model = "codellama:7b", 
    context_length = 16384,
    temperature = 0.1,
    top_p = 0.9,
    max_tokens = 2048,
    description = "ä»£ç ç”Ÿæˆä¸“ç”¨æ¨¡å‹"
}

# è½»é‡çº§ï¼ˆå¤‡ç”¨ï¼‰
phi_small = { 
    model = "phi:2.7b", 
    context_length = 2048,
    temperature = 0.8,
    max_tokens = 512,
    description = "è½»é‡çº§æ¨¡å‹ï¼Œèµ„æºå—é™ç¯å¢ƒ"
}

# ä¸­æ–‡æ”¯æŒ
qwen = { 
    model = "qwen:7b", 
    context_length = 32768,
    temperature = 0.7,
    max_tokens = 2048,
    description = "ä¸­æ–‡ä¼˜åŒ–æ¨¡å‹"
}

[models.ollama.default]
model = "llama2:7b-chat"
temperature = 0.7
max_tokens = 2048

# APIé…ç½®
[api]
rate_limit = 100  # æ¯åˆ†é’Ÿè¯·æ±‚é™åˆ¶
request_timeout = 300  # è¯·æ±‚è¶…æ—¶æ—¶é—´

# ç›‘æ§é…ç½®
[monitoring]
enabled = true
metrics_endpoint = "/metrics"
health_endpoint = "/health"
```

## ğŸ‰ æµ‹è¯•é…ç½®

### å¿«é€Ÿæµ‹è¯•è„šæœ¬

#### 1. Ollamaæµ‹è¯•è„šæœ¬
```python
# test_local_models.py
import asyncio
from agentbus.integrations.ollama import OllamaModel

async def test_ollama():
    # æµ‹è¯•Ollamaè¿æ¥
    model = OllamaModel(base_url="http://localhost:11434")
    
    # æµ‹è¯•å¯¹è¯
    response = await model.chat(
        model="llama2:7b",
        messages=[{"role": "user", "content": "Hello! How are you?"}],
        temperature=0.7
    )
    
    print("Ollamaæ¨¡å‹å“åº”:", response["choices"][0]["message"]["content"])

if __name__ == "__main__":
    asyncio.run(test_ollama())
```

#### 2. vLLMæµ‹è¯•è„šæœ¬ï¼ˆOpenAIå…¼å®¹ï¼‰
```python
# test_vllm_models.py
import asyncio
import openai
from openai import AsyncOpenAI

async def test_vllm():
    # è¿æ¥vLLM OpenAIå…¼å®¹API
    client = AsyncOpenAI(
        base_url="http://localhost:8001/v1",
        api_key="sk-vllm-local"
    )
    
    # æµ‹è¯•èŠå¤©å®Œæˆ
    response = await client.chat.completions.create(
        model="llama2-7b-chat",
        messages=[
            {"role": "system", "content": "You are a helpful AI assistant."},
            {"role": "user", "content": "å†™ä¸€ä¸ªPythonçš„Hello Worldç¨‹åº"}
        ],
        temperature=0.7,
        max_tokens=100
    )
    
    print("vLLMæ¨¡å‹å“åº”:", response.choices[0].message.content)
    
    # æµ‹è¯•æµå¼å“åº”
    print("\næµå¼å“åº”æµ‹è¯•:")
    async for chunk in await client.chat.completions.create(
        model="llama2-7b-chat",
        messages=[{"role": "user", "content": "è§£é‡Šä¸€ä¸‹ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½"}],
        stream=True,
        temperature=0.7
    ):
        if chunk.choices[0].delta.content is not None:
            print(chunk.choices[0].delta.content, end="", flush=True)

if __name__ == "__main__":
    asyncio.run(test_vllm())
```

#### 3. å®Œæ•´æµ‹è¯•å¥—ä»¶
```python
# test_all_models.py
import asyncio
import time
from agentbus.integrations.ollama import OllamaModel
from openai import AsyncOpenAI

class ModelTester:
    def __init__(self):
        self.ollama_model = OllamaModel(base_url="http://localhost:11434")
        self.vllm_client = AsyncOpenAI(
            base_url="http://localhost:8001/v1",
            api_key="sk-vllm-local"
        )
    
    async def test_ollama_models(self):
        """æµ‹è¯•Ollamaæ¨¡å‹"""
        print("=== æµ‹è¯•Ollamaæ¨¡å‹ ===")
        
        models = ["llama2:7b", "codellama:7b", "phi:2.7b"]
        
        for model in models:
            try:
                start_time = time.time()
                response = await self.ollama_model.chat(
                    model=model,
                    messages=[{"role": "user", "content": "ç®€çŸ­ä»‹ç»ä¸€ä¸‹è‡ªå·±"}],
                    temperature=0.7,
                    max_tokens=50
                )
                end_time = time.time()
                
                print(f"âœ… {model}: {response['choices'][0]['message']['content'][:50]}...")
                print(f"   å“åº”æ—¶é—´: {end_time - start_time:.2f}ç§’")
            except Exception as e:
                print(f"âŒ {model}: {e}")
            print()
    
    async def test_vllm_models(self):
        """æµ‹è¯•vLLMæ¨¡å‹"""
        print("=== æµ‹è¯•vLLMæ¨¡å‹ ===")
        
        models = ["llama2-7b-chat", "codellama-7b-instruct", "mistral-7b-instruct-v0.1"]
        
        for model in models:
            try:
                start_time = time.time()
                response = await self.vllm_client.chat.completions.create(
                    model=model,
                    messages=[{"role": "user", "content": "ç®€çŸ­ä»‹ç»ä¸€ä¸‹è‡ªå·±"}],
                    temperature=0.7,
                    max_tokens=50
                )
                end_time = time.time()
                
                print(f"âœ… {model}: {response.choices[0].message.content[:50]}...")
                print(f"   å“åº”æ—¶é—´: {end_time - start_time:.2f}ç§’")
            except Exception as e:
                print(f"âŒ {model}: {e}")
            print()
    
    async def test_performance(self):
        """æ€§èƒ½æµ‹è¯•"""
        print("=== æ€§èƒ½å¯¹æ¯”æµ‹è¯• ===")
        
        # æµ‹è¯•Ollamaå¹¶å‘æ€§èƒ½
        print("Ollamaå¹¶å‘æµ‹è¯•...")
        start_time = time.time()
        tasks = [
            self.ollama_model.chat(
                model="llama2:7b",
                messages=[{"role": "user", "content": f"å›ç­”ç¬¬{i}ä¸ªé—®é¢˜"}],
                temperature=0.7,
                max_tokens=20
            ) for i in range(5)
        ]
        await asyncio.gather(*tasks)
        ollama_time = time.time() - start_time
        print(f"Ollama 5ä¸ªå¹¶å‘è¯·æ±‚å®Œæˆæ—¶é—´: {ollama_time:.2f}ç§’")
        
        # æµ‹è¯•vLLMå¹¶å‘æ€§èƒ½
        print("vLLMå¹¶å‘æµ‹è¯•...")
        start_time = time.time()
        tasks = [
            self.vllm_client.chat.completions.create(
                model="llama2-7b-chat",
                messages=[{"role": "user", "content": f"å›ç­”ç¬¬{i}ä¸ªé—®é¢˜"}],
                temperature=0.7,
                max_tokens=20
            ) for i in range(5)
        ]
        await asyncio.gather(*tasks)
        vllm_time = time.time() - start_time
        print(f"vLLM 5ä¸ªå¹¶å‘è¯·æ±‚å®Œæˆæ—¶é—´: {vllm_time:.2f}ç§’")
        
        print(f"æ€§èƒ½å¯¹æ¯”: vLLMæ¯”Ollamaå¿« {((ollama_time - vllm_time) / ollama_time * 100):.1f}%")

async def main():
    tester = ModelTester()
    
    print("AgentBus æœ¬åœ°æ¨¡å‹æµ‹è¯•å¥—ä»¶")
    print("=" * 50)
    
    # æ£€æŸ¥æœåŠ¡çŠ¶æ€
    print("æ£€æŸ¥æœåŠ¡çŠ¶æ€...")
    try:
        await tester.test_ollama_models()
    except Exception as e:
        print(f"OllamaæœåŠ¡ä¸å¯ç”¨: {e}")
    
    try:
        await tester.test_vllm_models()
    except Exception as e:
        print(f"vLLMæœåŠ¡ä¸å¯ç”¨: {e}")
    
    # æ€§èƒ½å¯¹æ¯”ï¼ˆå¦‚æœä¸¤ä¸ªæœåŠ¡éƒ½å¯ç”¨ï¼‰
    try:
        await tester.test_performance()
    except Exception as e:
        print(f"æ€§èƒ½æµ‹è¯•è·³è¿‡: {e}")
    
    print("\næµ‹è¯•å®Œæˆï¼")

if __name__ == "__main__":
    asyncio.run(main())
```

### è¿è¡Œæµ‹è¯•

```bash
# æµ‹è¯•Ollamaæ¨¡å‹
python test_local_models.py

# æµ‹è¯•vLLMæ¨¡å‹
python test_vllm_models.py

# è¿è¡Œå®Œæ•´æµ‹è¯•å¥—ä»¶
python test_all_models.py

# é¢„æœŸè¾“å‡ºç¤ºä¾‹ï¼š
# AgentBus æœ¬åœ°æ¨¡å‹æµ‹è¯•å¥—ä»¶
# ==================================================
# æ£€æŸ¥æœåŠ¡çŠ¶æ€...
# === æµ‹è¯•Ollamaæ¨¡å‹ ===
# âœ… llama2:7b: Hello! I'm doing great, thank you for asking. How can I help you today? ...
#    å“åº”æ—¶é—´: 2.34ç§’
# 
# âœ… codellama:7b: I'm an AI assistant designed to help with programming and coding tasks...
#    å“åº”æ—¶é—´: 3.12ç§’
# 
# === æµ‹è¯•vLLMæ¨¡å‹ ===
# âœ… llama2-7b-chat: Hello! I'm an AI assistant created by Meta. I'm here to help...
#    å“åº”æ—¶é—´: 1.45ç§’
# 
# æ€§èƒ½å¯¹æ¯”: vLLMæ¯”Ollamaå¿« 38.1%
```

## ğŸ“š æ›´å¤šèµ„æº

- [Ollamaå®˜æ–¹æ–‡æ¡£](https://ollama.ai/docs)
- [LM Studioæ–‡æ¡£](https://lmstudio.ai/docs)
- [Hugging Faceæ¨¡å‹åº“](https://huggingface.co/models)
- [GPT4Allæ–‡æ¡£](https://docs.gpt4all.io/)
- [vLLM GitHubä»“åº“](https://github.com/vllm-project/vllm)
- [vLLMå®˜æ–¹æ–‡æ¡£](https://docs.vllm.ai/)

## ğŸ¯ å¿«é€Ÿå¯¼èˆª

| éœ€æ±‚ | æ¨èæ–¹æ¡ˆ | é…ç½®æ–‡ä»¶ | å¿«é€Ÿå‘½ä»¤ |
|------|----------|----------|----------|
| **æœ€ç®€å•çš„æœ¬åœ°æ¨¡å‹** | Ollama | `config/ollama.toml` | `ollama serve` |
| **é«˜æ€§èƒ½ç”Ÿäº§ç¯å¢ƒ** | vLLM (OpenAIæ ¼å¼) | `config/vllm.toml` | `python -m vllm.entrypoints.openai.api_server` |
| **å›¾å½¢ç•Œé¢ç®¡ç†** | LM Studio | `config/lmstudio.toml` | LM Studio GUIå¯åŠ¨API |
| **å®Œå…¨ç¦»çº¿** | GPT4All | `config/gpt4all.py` | Python APIè°ƒç”¨ |
| **ä»£ç ç”Ÿæˆä¸“ç”¨** | Code Llama (vLLM) | `config/codellama.toml` | `--model codellama/CodeLlama-7b-Instruct-hf` |
| **ä¸­æ–‡ä¼˜åŒ–** | Qwen (Ollama) | `config/qwen.toml` | `ollama pull qwen:7b` |
| **ä½é…ç½®è®¾å¤‡** | Phi-2.7B (Ollama) | `config/phi.toml` | `ollama pull phi:2.7b` |

## ğŸ† é€‰æ‹©å»ºè®®

### å¼€å‘ç¯å¢ƒæ¨è
- **é¦–é€‰**: Ollama + Llama2 7Bï¼ˆç®€å•æ˜“ç”¨ï¼‰
- **å¤‡é€‰**: vLLM + Llama2 7Bï¼ˆæ€§èƒ½æ›´å¥½ï¼‰

### ç”Ÿäº§ç¯å¢ƒæ¨è
- **é¦–é€‰**: vLLM + ä¼˜åŒ–æ¨¡å‹ï¼ˆé«˜æ€§èƒ½ã€OpenAIå…¼å®¹ï¼‰
- **å¤‡é€‰**: Ollama + è½»é‡çº§æ¨¡å‹ï¼ˆç¨³å®šå¯é ï¼‰

### å›¢é˜Ÿåä½œæ¨è
- **é¦–é€‰**: vLLMï¼ˆæ”¯æŒå¹¶å‘ã€å¤šæ¨¡å‹ï¼‰
- **å¤‡é€‰**: Ollamaï¼ˆç®€å•éƒ¨ç½²ã€ç»´æŠ¤å®¹æ˜“ï¼‰

---

## âœ… å¿«é€Ÿé…ç½®æ€»ç»“

### æ–¹æ¡ˆ1ï¼šOllamaï¼ˆæ¨èæ–°æ‰‹ï¼‰
1. å®‰è£…Ollamaï¼š`curl -fsSL https://ollama.ai/install.sh | sh`
2. ä¸‹è½½æ¨¡å‹ï¼š`ollama pull codellama:7b`
3. å¯åŠ¨æœåŠ¡ï¼š`ollama serve`
4. åˆ›å»ºé…ç½®æ–‡ä»¶ï¼š`config/ollama.toml`
5. æµ‹è¯•è¿æ¥ï¼š`python test_local_models.py`

### æ–¹æ¡ˆ2ï¼švLLMï¼ˆæ¨èç”Ÿäº§ï¼‰
1. å®‰è£…vLLMï¼š`pip install vllm`
2. å¯åŠ¨æœåŠ¡ï¼š`python -m vllm.entrypoints.openai.api_server --model meta-llama/Llama-2-7b-chat-hf`
3. åˆ›å»ºé…ç½®æ–‡ä»¶ï¼š`config/vllm.toml`
4. æµ‹è¯•è¿æ¥ï¼š`python test_vllm_models.py`

### æ¨èæ¨¡å‹é…ç½®
| ç”¨é€” | æ¨¡å‹ | å†…å­˜éœ€æ±‚ | ç‰¹ç‚¹ |
|------|------|----------|------|
| **ä»£ç ç”Ÿæˆ** | `codellama:7b` | 8GB+ | ä¸“ä¸šçš„ä»£ç ç†è§£èƒ½åŠ› |
| **æœ€æ–°ä»£ç æ¨¡å‹** | `qwen2.5-coder:7b` | 8GB+ | 2024å¹´æœ€æ–°ä»£ç ä¸“å®¶ |
| **é€šç”¨å¯¹è¯** | `llama3.1:8b` | 8GB+ | Metaæœ€æ–°Llama 3.1 |
| **ä¸­æ–‡æ”¯æŒ** | `qwen2.5:7b` | 8GB+ | æœ€æ–°çš„ä¸­æ–‡ç†è§£æ¨¡å‹ |
| **è½»é‡çº§** | `phi3:medium` | 4GB+ | Microsoftæœ€æ–°è½»é‡æ¨¡å‹ |
| **é«˜æ€§èƒ½** | `llama3.1:70b` (vLLM) | 40GB+ | é¡¶çº§ç†è§£èƒ½åŠ› |
| **å¤šè¯­è¨€** | `mistral-nemo:12b` | 12GB+ | Mistralæœ€æ–°å¤šè¯­è¨€æ¨¡å‹ |

### æ€§èƒ½åŸºå‡†
| å¹³å° | å•è¯·æ±‚å“åº”æ—¶é—´ | å¹¶å‘æ€§èƒ½ | å†…å­˜ä½¿ç”¨ | æ¨èåœºæ™¯ |
|------|---------------|----------|----------|----------|
| **Ollama** | 2-4ç§’ | ä¸€èˆ¬ | 4-8GB | å¼€å‘æµ‹è¯•ã€ç®€å•éƒ¨ç½² |
| **vLLM** | 1-2ç§’ | ä¼˜ç§€ | 6-12GB | ç”Ÿäº§ç¯å¢ƒã€é«˜å¹¶å‘ |
| **LM Studio** | 3-5ç§’ | ä¸€èˆ¬ | 4-8GB | å›¾å½¢åŒ–ç®¡ç†ã€å°å›¢é˜Ÿ |
| **GPT4All** | 5-10ç§’ | å·® | 2-4GB | å®Œå…¨ç¦»çº¿ã€æç®€éœ€æ±‚ |

**æœ€åæ›´æ–°**: 2026-01-29  
**ç‰ˆæœ¬**: v1.0 - å®Œæ•´æ”¯æŒvLLM OpenAIæ ¼å¼