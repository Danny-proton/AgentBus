---
AIGC:
    ContentProducer: Minimax Agent AI
    ContentPropagator: Minimax Agent AI
    Label: AIGC
    ProduceID: "00000000000000000000000000000000"
    PropagateID: "00000000000000000000000000000000"
    ReservedCode1: 304502204ad3933730a5f1e8e2eb9cc62cc7b0717663a71fc8e43d8f8bab41923bb4dd10022100849ce965208a78d7ec849c5c7b55b75ebc0b21791a3e52b8204c137c1ff1d501
    ReservedCode2: 30450220513a616d8f8659a43c00fab0ef04868a17085592b3a0d6185107176c4dfe3e230221008eb1d46d715b2da90364f16b87e57fd73a155168c05948748d4baa615381d716
---

# AgentBus å®Œæ•´éƒ¨ç½²æŒ‡å—

## ğŸ“‹ æ¦‚è¿°

AgentBusæ˜¯ç”±è¡Œä¸šè§£å†³æ–¹æ¡ˆé›†æˆä¸éªŒè¯éƒ¨ï¼ˆè¡Œè§£ï¼‰ç‰µå¤´æ‰“é€ çš„æ™ºèƒ½æµ‹è¯•åä½œåŠ©æ‰‹å¹³å°ï¼Œé€šè¿‡æ’ä»¶åŒ–æ¶æ„æä¾›å¤šæ¸ é“æ²Ÿé€šã€æ™ºèƒ½ä»»åŠ¡è°ƒåº¦ã€è‡ªåŠ¨åŒ–æµ‹è¯•æ‰§è¡Œå’Œå›¢é˜Ÿåä½œåŠŸèƒ½ã€‚æ”¯æŒäº‘ç«¯å’Œæœ¬åœ°AIæ¨¡å‹é›†æˆï¼Œè®©å›¢é˜Ÿåä½œæ›´é«˜æ•ˆã€æµ‹è¯•æµç¨‹æ›´æ™ºèƒ½ã€‚æœ¬æŒ‡å—å°†è¯¦ç»†ä»‹ç»å¦‚ä½•åœ¨å„ç§ç¯å¢ƒä¸­éƒ¨ç½²å’Œé…ç½®AgentBusã€‚

## ğŸš€ å¿«é€Ÿå¼€å§‹

### ç¯å¢ƒè¦æ±‚

- **Python**: 3.8+
- **æ“ä½œç³»ç»Ÿ**: Linux, macOS, Windows
- **å†…å­˜**: æœ€å°‘ 2GBï¼Œæ¨è 4GB+
- **ç£ç›˜ç©ºé—´**: 1GB+

### å®‰è£…æ­¥éª¤

1. **å…‹éš†é¡¹ç›®**
```bash
git clone <repository-url>
cd agentbus
```

2. **å®‰è£…ä¾èµ–**
```bash
pip install -r requirements.txt
```

3. **å®‰è£…å¯é€‰ä¾èµ–**
```bash
# å¦‚æœéœ€è¦ä½¿ç”¨æµè§ˆå™¨è‡ªåŠ¨åŒ–
pip install playwright
playwright install

# å¦‚æœéœ€è¦ä½¿ç”¨å‘é‡æ•°æ®åº“
pip install lancedb

# å¦‚æœéœ€è¦æœ¬åœ°AIæ¨¡å‹æ”¯æŒ
pip install torch transformers accelerate
```

4. **åˆå§‹åŒ–é…ç½®**
```bash
# åˆ›å»ºé»˜è®¤é…ç½®æ–‡ä»¶
python -c "from agentbus.config.settings import setup_default_config; setup_default_config()"

# åˆ›å»ºå·¥ä½œç©ºé—´ç›®å½•
mkdir -p workspace/{logs,scripts,plans,contexts,temp,memory,knowledge,agent}
```

5. **å¯åŠ¨æœåŠ¡**
```bash
# Webæ¨¡å¼ï¼ˆæ¨èï¼‰
python -m agentbus.core.app --mode web --host 0.0.0.0 --port 8000

# å¼€å‘æ¨¡å¼
python -m agentbus.core.app --mode dev --reload

# CLIæ¨¡å¼
python -m agentbus.core.app --mode cli
```

## ğŸ—ï¸ ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²

### ä½¿ç”¨Dockeréƒ¨ç½²

1. **æ„å»ºDockeré•œåƒ**
```bash
docker build -t agentbus:latest .
```

2. **è¿è¡Œå®¹å™¨**
```bash
docker run -d \
  --name agentbus \
  -p 8000:8000 \
  -v $(pwd)/workspace:/app/workspace \
  -v $(pwd)/config:/app/config \
  agentbus:latest
```

3. **ä½¿ç”¨Docker Compose**
```yaml
# docker-compose.yml
version: '3.8'
services:
  agentbus:
    build: .
    ports:
      - "8000:8000"
    volumes:
      - ./workspace:/app/workspace
      - ./config:/app/config
      - ./data:/app/data
    environment:
      - ENVIRONMENT=production
      - DEBUG=false
      - LOG_LEVEL=info
    restart: unless-stopped

  # å¯é€‰ï¼šRedisç”¨äºç¼“å­˜
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data

volumes:
  redis_data:
```

## âš™ï¸ é…ç½®ç®¡ç†

### ç¯å¢ƒå˜é‡é…ç½®

```bash
# .envæ–‡ä»¶
ENVIRONMENT=production
DEBUG=false
LOG_LEVEL=info

# APIé…ç½®
API_HOST=0.0.0.0
API_PORT=8000
API_WORKERS=4

# å®‰å…¨é…ç½®
SECRET_KEY=your-secret-key-here
ALLOWED_HOSTS=localhost,127.0.0.1,your-domain.com

# æ•°æ®åº“é…ç½®
DATABASE_URL=sqlite:///./data/agentbus.db

# AIæ¨¡å‹é…ç½®
DEFAULT_MODEL_PROVIDER=openai
OPENAI_API_KEY=your-openai-key
ANTHROPIC_API_KEY=your-anthropic-key
```

### é…ç½®æ–‡ä»¶

AgentBusæ”¯æŒTOMLæ ¼å¼çš„é…ç½®æ–‡ä»¶ï¼š

```toml
# config/production.toml
[server]
host = "0.0.0.0"
port = 8000
workers = 4
reload = false

[database]
url = "sqlite:///./data/agentbus.db"
pool_size = 10
max_overflow = 20

[logging]
level = "info"
file = "logs/agentbus.log"
max_size = "100MB"
backup_count = 5

[security]
secret_key = "${SECRET_KEY}"
session_timeout = 3600
max_request_size = "10MB"

[plugins]
auto_load = true
directories = [
    "./plugins",
    "./custom_plugins"
]

[channels]
auto_connect = false
config_file = "./config/channels.yaml"
```

## ğŸ¤– æœ¬åœ°AIæ¨¡å‹é…ç½®

AgentBusæ”¯æŒå¤šç§æœ¬åœ°AIæ¨¡å‹ï¼Œä»¥ä¸‹æ˜¯è¯¦ç»†é…ç½®æŒ‡å—ï¼š

### 1. Ollamaé…ç½®ï¼ˆæ¨èï¼‰

```bash
# 1. å®‰è£…Ollama
curl -fsSL https://ollama.ai/install.sh | sh

# 2. å¯åŠ¨OllamaæœåŠ¡
ollama serve

# 3. ä¸‹è½½æ¨¡å‹
ollama pull llama2
ollama pull codellama
ollama pull mistral
ollama pull phi

# 4. é…ç½®AgentBus
```

```toml
# config/models.toml
[models.local]
provider = "ollama"
base_url = "http://localhost:11434"
timeout = 120

[models.local.models]
llama2 = { model = "llama2:7b", context_length = 4096 }
codellama = { model = "codellama:7b", context_length = 16384 }
mistral = { model = "mistral:7b", context_length = 32768 }
phi = { model = "phi:2.7b", context_length = 2048 }

[models.local.default]
model = "llama2:7b"
temperature = 0.7
max_tokens = 2048
```

### 2. vLLMé…ç½®

```bash
# 1. å®‰è£…vLLM
pip install vllm

# 2. å¯åŠ¨vLLMæœåŠ¡å™¨
python -m vllm.entrypoints.openai.api_server \
    --model microsoft/DialoGPT-medium \
    --host 0.0.0.0 \
    --port 8001

# 3. é…ç½®AgentBus
```

```toml
[models.vllm]
provider = "vllm"
base_url = "http://localhost:8001/v1"
api_key = "sk-vllm"  # è™šæ‹ŸAPI key

[models.vllm.models]
gpt = { model = "microsoft/DialoGPT-medium" }
```

### 3. LM Studioé…ç½®

```bash
# 1. ä¸‹è½½å¹¶å¯åŠ¨LM Studio
# 2. åŠ è½½æœ¬åœ°æ¨¡å‹
# 3. å¯åŠ¨æœ¬åœ°APIæœåŠ¡å™¨ï¼ˆé»˜è®¤ç«¯å£1234ï¼‰
```

```toml
[models.lmstudio]
provider = "openai"
base_url = "http://localhost:1234/v1"
api_key = "sk-lmstudio"

[models.lmstudio.models]
local = { model = "local-model" }
```

### 4. GPT4Allé…ç½®

```bash
# 1. å®‰è£…GPT4All
pip install gpt4all

# 2. ä¸‹è½½æ¨¡å‹
from gpt4all import GPT4All
GPT4All.download_model('ggml-model-gpt4all-falcon.bin')

# 3. é…ç½®AgentBus
```

```python
# config/gpt4all_config.py
from agentbus.integrations.gpt4all import GPT4AllModel

config = {
    'model_path': './models/ggml-model-gpt4all-falcon.bin',
    'device': 'cpu',  # æˆ– 'cuda'
    'n_threads': 4,
    'n_ctx': 2048,
    'temperature': 0.7,
    'max_tokens': 512
}

model = GPT4AllModel(config)
```

### 5. Hugging Face Transformersé…ç½®

```python
# config/hf_config.py
from agentbus.integrations.huggingface import HFModel

config = {
    'model_name': 'microsoft/DialoGPT-medium',
    'device': 'auto',  # è‡ªåŠ¨æ£€æµ‹GPU
    'torch_dtype': 'auto',
    'trust_remote_code': True,
    'load_in_8bit': False,
    'load_in_4bit': False
}

model = HFModel(config)
```

### 6. å®Œæ•´çš„æœ¬åœ°æ¨¡å‹é…ç½®ç¤ºä¾‹

```toml
# config/local_models.toml
[models]
default_provider = "ollama"

[models.ollama]
provider = "ollama"
base_url = "http://localhost:11434"
timeout = 300

[models.ollama.models]
codellama = { 
    model = "codellama:7b", 
    context_length = 16384,
    temperature = 0.1,
    top_p = 0.9
}
llama2 = { 
    model = "llama2:7b", 
    context_length = 4096,
    temperature = 0.7
}
mistral = { 
    model = "mistral:7b", 
    context_length = 32768,
    temperature = 0.8
}

[models.fallback]
provider = "huggingface"
model_name = "microsoft/DialoGPT-medium"
device = "cpu"
```

### æ¨¡å‹é€‰æ‹©å»ºè®®

| éœ€æ±‚ | æ¨èæ¨¡å‹ | é…ç½®è¦ç‚¹ |
|------|----------|----------|
| ä»£ç ç”Ÿæˆ | CodeLlama, StarCoder | é«˜æ¸©åº¦ï¼ŒçŸ­ä¸Šä¸‹æ–‡ |
| å¯¹è¯ç³»ç»Ÿ | Llama2, Mistral | ä¸­ç­‰æ¸©åº¦ï¼Œé•¿ä¸Šä¸‹æ–‡ |
| æ–‡æœ¬æ€»ç»“ | T5, BART | ä½æ¸©åº¦ï¼Œé€‚ä¸­ä¸Šä¸‹æ–‡ |
| åˆ›æ„å†™ä½œ | GPT-NeoX, PaLM | é«˜æ¸©åº¦ï¼Œé•¿ä¸Šä¸‹æ–‡ |
| èµ„æºå—é™ | Phi, Qwen | å°æ¨¡å‹ï¼ŒCPUè¿è¡Œ |

## ğŸ”§ é«˜çº§é…ç½®

### æ’ä»¶ç³»ç»Ÿé…ç½®

```toml
[plugins]
auto_load = true
auto_activate = true
directories = [
    "./plugins/official",
    "./plugins/custom",
    "/usr/local/share/agentbus/plugins"
]

[plugins.github]
enabled = true
api_token = "${GITHUB_TOKEN}"
rate_limit = 5000

[plugins.browser]
enabled = true
headless = false
timeout = 30000
```

### æ¸ é“ç³»ç»Ÿé…ç½®

```yaml
# config/channels.yaml
channels:
  telegram:
    enabled: true
    bot_token: "${TELEGRAM_BOT_TOKEN}"
    webhook_url: "${TELEGRAM_WEBHOOK_URL}"
    
  discord:
    enabled: true
    bot_token: "${DISCORD_BOT_TOKEN}"
    application_id: "${DISCORD_APP_ID}"
    
  slack:
    enabled: true
    bot_token: "${SLACK_BOT_TOKEN}"
    signing_secret = "${SLACK_SIGNING_SECRET}"
```

### è®°å¿†ç³»ç»Ÿé…ç½®

```toml
[memory]
enabled = true
backend = "hybrid"  # hybrid, sqlite, redis, lancedb

[memory.hybrid]
vector_backend = "lancedb"
text_backend = "sqlite"
relevance_threshold = 0.7

[memory.sqlite]
database_url = "sqlite:///./data/memory.db"
max_entries = 100000

[memory.lancedb]
persist_directory = "./data/vector_db"
collection_name = "agentbus_memory"
```

## ğŸ” ç›‘æ§å’Œæ—¥å¿—

### æ—¥å¿—é…ç½®

```toml
[logging]
level = "info"
format = "json"
file = "logs/agentbus.log"
max_size = "100MB"
backup_count = 5

[logging.console]
enabled = true
format = "text"
level = "info"

[logging.remote]
enabled = false
endpoint = "http://localhost:9000"
api_key = "${LOG_API_KEY}"
```

### ç›‘æ§é…ç½®

```toml
[monitoring]
enabled = true
metrics_port = 9090

[monitoring.health]
endpoint = "/health"
interval = 30

[monitoring.prometheus]
enabled = true
endpoint = "/metrics"
```

## ğŸ› ï¸ æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **ç«¯å£è¢«å ç”¨**
```bash
# æŸ¥æ‰¾å ç”¨è¿›ç¨‹
lsof -i :8000

# æ€æ­»è¿›ç¨‹
kill -9 <PID>
```

2. **æƒé™é—®é¢˜**
```bash
# åˆ›å»ºä¸“ç”¨ç”¨æˆ·
sudo useradd -r -s /bin/false agentbus
sudo chown -R agentbus:agentbus /opt/agentbus
```

3. **ä¾èµ–é—®é¢˜**
```bash
# é‡æ–°å®‰è£…ä¾èµ–
pip install --force-reinstall -r requirements.txt
```

4. **é…ç½®é—®é¢˜**
```bash
# éªŒè¯é…ç½®
python -c "from agentbus.config.settings import validate_config; validate_config()"
```

### è°ƒè¯•æ¨¡å¼

```bash
# å¯ç”¨è°ƒè¯•æ—¥å¿—
export LOG_LEVEL=debug

# å¯ç”¨è¯¦ç»†é”™è¯¯ä¿¡æ¯
export DEBUG=true

# å¯ç”¨çƒ­é‡è½½
python -m agentbus.core.app --mode dev --reload
```

## ğŸ“Š æ€§èƒ½ä¼˜åŒ–

### ç”Ÿäº§ç¯å¢ƒä¼˜åŒ–

1. **ä½¿ç”¨Gunicorn + Uvicorn**
```bash
pip install gunicorn uvicorn

gunicorn agentbus.core.app:create_app \
    -k uvicorn.workers.UvicornWorker \
    --bind 0.0.0.0:8000 \
    --workers 4 \
    --timeout 300
```

2. **æ•°æ®åº“ä¼˜åŒ–**
```sql
-- SQLiteä¼˜åŒ–
PRAGMA journal_mode = WAL;
PRAGMA synchronous = NORMAL;
PRAGMA cache_size = 10000;
PRAGMA temp_store = memory;
```

3. **å†…å­˜ä¼˜åŒ–**
```toml
[performance]
max_memory_usage = "2GB"
gc_threshold = 700
connection_pool_size = 20
```

## ğŸ” å®‰å…¨é…ç½®

### å®‰å…¨æœ€ä½³å®è·µ

1. **ä½¿ç”¨HTTPS**
```bash
# ä½¿ç”¨Let's Encrypt
certbot --nginx -d your-domain.com

# é…ç½®åå‘ä»£ç†
location / {
    proxy_pass http://localhost:8000;
    proxy_set_header Host $host;
    proxy_set_header X-Real-IP $remote_addr;
    proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
    proxy_set_header X-Forwarded-Proto $scheme;
}
```

2. **é˜²ç«å¢™é…ç½®**
```bash
# Ubuntu/Debian
sudo ufw allow 8000/tcp
sudo ufw enable

# CentOS/RHEL
sudo firewall-cmd --permanent --add-port=8000/tcp
sudo firewall-cmd --reload
```

3. **APIå¯†é’¥ç®¡ç†**
```bash
# ä½¿ç”¨å¯†é’¥ç®¡ç†æœåŠ¡
export OPENAI_API_KEY="$(vault kv get -field=api_key secret/agentbus/openai)"
export ANTHROPIC_API_KEY="$(vault kv get -field=api_key secret/agentbus/anthropic)"
```

## ğŸš€ å‡çº§å’Œå¤‡ä»½

### ç‰ˆæœ¬å‡çº§

```bash
# å¤‡ä»½æ•°æ®
cp -r ./data ./data.backup.$(date +%Y%m%d)

# å‡çº§ä»£ç 
git pull origin main

# å‡çº§ä¾èµ–
pip install --upgrade -r requirements.txt

# è¿è¡Œæ•°æ®åº“è¿ç§»
python -m agentbus.migrations.upgrade
```

### å¤‡ä»½ç­–ç•¥

```bash
#!/bin/bash
# backup.sh

DATE=$(date +%Y%m%d_%H%M%S)
BACKUP_DIR="/backup/agentbus"

# å¤‡ä»½æ•°æ®
mkdir -p $BACKUP_DIR
tar -czf $BACKUP_DIR/data_$DATE.tar.gz ./data

# å¤‡ä»½é…ç½®
tar -czf $BACKUP_DIR/config_$DATE.tar.gz ./config

# å¤‡ä»½å·¥ä½œç©ºé—´
tar -czf $BACKUP_DIR/workspace_$DATE.tar.gz ./workspace

# æ¸…ç†æ—§å¤‡ä»½ï¼ˆä¿ç•™30å¤©ï¼‰
find $BACKUP_DIR -name "*.tar.gz" -mtime +30 -delete
```

## ğŸ“ æ”¯æŒå’Œå¸®åŠ©

### è·å–å¸®åŠ©

- **æ–‡æ¡£**: [é¡¹ç›®æ–‡æ¡£é“¾æ¥]
- **Issues**: [GitHub Issuesé“¾æ¥]
- **è®¨è®º**: [GitHub Discussionsé“¾æ¥]
- **ç¤¾åŒº**: [Discord/å¾®ä¿¡ç¾¤é“¾æ¥]

### è”ç³»ä¿¡æ¯

- **é‚®ç®±**: support@agentbus.com
- **ç½‘ç«™**: https://agentbus.com
- **GitHub**: https://github.com/agentbus/agentbus

---

**æœ€åæ›´æ–°**: 2026-01-29  
**ç‰ˆæœ¬**: 1.0.0  
**ä½œè€…**: AgentBus Team