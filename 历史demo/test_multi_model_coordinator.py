#!/usr/bin/env python3
"""
å¤šæ¨¡å‹åè°ƒå™¨æµ‹è¯•è„šæœ¬
Multi-Model Coordinator Test Script

æµ‹è¯•å¤šæ¨¡å‹åè°ƒå™¨çš„å„é¡¹åŠŸèƒ½ï¼ŒåŒ…æ‹¬æ¨¡å‹æ³¨å†Œã€ä»»åŠ¡æäº¤ã€ç»“æœæŸ¥è¯¢ç­‰ã€‚
"""

import asyncio
import json
import time
from datetime import datetime
from agentbus.services.multi_model_coordinator import (
    MultiModelCoordinator,
    ModelConfig,
    TaskRequest,
    TaskType,
    TaskPriority,
    ModelType,
)
from agentbus.core.settings import settings


async def test_multi_model_coordinator():
    """æµ‹è¯•å¤šæ¨¡å‹åè°ƒå™¨åŠŸèƒ½"""
    
    print("ğŸš€ AgentBus å¤šæ¨¡å‹åè°ƒå™¨æµ‹è¯•")
    print("=" * 50)
    print(f"æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print()
    
    # 1. åˆå§‹åŒ–åè°ƒå™¨
    print("ğŸ“‹ æ­¥éª¤ 1: åˆå§‹åŒ–å¤šæ¨¡å‹åè°ƒå™¨")
    coordinator = MultiModelCoordinator()
    await coordinator.initialize()
    print("âœ… å¤šæ¨¡å‹åè°ƒå™¨åˆå§‹åŒ–å®Œæˆ")
    
    # 2. è·å–é»˜è®¤æ¨¡å‹
    print("\nğŸ“‹ æ­¥éª¤ 2: æŸ¥çœ‹é»˜è®¤æ³¨å†Œæ¨¡å‹")
    available_models = coordinator.get_available_models()
    print(f"âœ… é»˜è®¤æ³¨å†Œäº† {len(available_models)} ä¸ªæ¨¡å‹:")
    for model in available_models:
        print(f"   - {model.model_id} ({model.model_name}) - {model.provider}")
    
    # 3. æ³¨å†Œè‡ªå®šä¹‰æ¨¡å‹
    print("\nğŸ“‹ æ­¥éª¤ 3: æ³¨å†Œè‡ªå®šä¹‰æ¨¡å‹")
    custom_model = ModelConfig(
        model_id="gpt-3.5-turbo",
        model_name="GPT-3.5 Turbo",
        model_type=ModelType.TEXT_GENERATION,
        provider="openai",
        capabilities=[TaskType.TEXT_GENERATION, TaskType.QUESTION_ANSWERING],
        cost_per_token=0.000002,
        quality_score=0.88,
        max_tokens=4096,
        temperature=0.7
    )
    coordinator.register_model(custom_model)
    print("âœ… è‡ªå®šä¹‰æ¨¡å‹æ³¨å†Œå®Œæˆ")
    
    # 4. æäº¤ä»»åŠ¡
    print("\nğŸ“‹ æ­¥éª¤ 4: æäº¤å„ç§ç±»å‹ä»»åŠ¡")
    
    # æ–‡æœ¬ç”Ÿæˆä»»åŠ¡
    text_gen_task = TaskRequest(
        task_id="text_gen_001",
        task_type=TaskType.TEXT_GENERATION,
        content="è¯·å†™ä¸€æ®µå…³äºäººå·¥æ™ºèƒ½çš„ä»‹ç»",
        priority=TaskPriority.NORMAL,
        required_capabilities=[TaskType.TEXT_GENERATION],
        max_cost=0.01
    )
    task_id_1 = await coordinator.submit_task(text_gen_task)
    print(f"âœ… æ–‡æœ¬ç”Ÿæˆä»»åŠ¡å·²æäº¤: {task_id_1}")
    
    # ä»£ç ç”Ÿæˆä»»åŠ¡
    code_gen_task = TaskRequest(
        task_id="code_gen_001", 
        task_type=TaskType.CODE_GENERATION,
        content="è¯·å†™ä¸€ä¸ªPythonçš„Hello Worldç¨‹åº",
        priority=TaskPriority.HIGH,
        required_capabilities=[TaskType.CODE_GENERATION],
        max_cost=0.005
    )
    task_id_2 = await coordinator.submit_task(code_gen_task)
    print(f"âœ… ä»£ç ç”Ÿæˆä»»åŠ¡å·²æäº¤: {task_id_2}")
    
    # é—®ç­”ä»»åŠ¡
    qa_task = TaskRequest(
        task_id="qa_001",
        task_type=TaskType.QUESTION_ANSWERING,
        content="ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ",
        priority=TaskPriority.NORMAL,
        required_capabilities=[TaskType.QUESTION_ANSWERING],
        max_cost=0.003
    )
    task_id_3 = await coordinator.submit_task(qa_task)
    print(f"âœ… é—®ç­”ä»»åŠ¡å·²æäº¤: {task_id_3}")
    
    # 5. ç­‰å¾…ä»»åŠ¡å®Œæˆ
    print("\nğŸ“‹ æ­¥éª¤ 5: ç­‰å¾…ä»»åŠ¡å¤„ç†å®Œæˆ")
    
    tasks_to_check = [task_id_1, task_id_2, task_id_3]
    max_wait_time = 30  # æœ€å¤šç­‰å¾…30ç§’
    start_time = time.time()
    
    while tasks_to_check and (time.time() - start_time) < max_wait_time:
        completed_tasks = []
        
        for task_id in tasks_to_check:
            result = await coordinator.get_task_result(task_id)
            if result:
                if result.status.value == "completed":
                    completed_tasks.append(task_id)
                    print(f"âœ… ä»»åŠ¡ {task_id} å·²å®Œæˆ")
                    print(f"   ç»“æœ: {result.final_content[:100]}...")
                    print(f"   ä½¿ç”¨æ¨¡å‹: {[r.model_id for r in result.model_results]}")
                    print(f"   å¤„ç†æ—¶é—´: {result.total_time:.2f}s")
                    print(f"   æˆæœ¬: ${result.total_cost:.6f}")
                elif result.status.value == "failed":
                    completed_tasks.append(task_id)
                    print(f"âŒ ä»»åŠ¡ {task_id} å¤±è´¥: {result.error}")
        
        # ç§»é™¤å·²å®Œæˆçš„ä»»åŠ¡
        for task_id in completed_tasks:
            tasks_to_check.remove(task_id)
        
        if tasks_to_check:
            print(f"â³ è¿˜æœ‰ {len(tasks_to_check)} ä¸ªä»»åŠ¡åœ¨å¤„ç†ä¸­...")
            await asyncio.sleep(2)
    
    if tasks_to_check:
        print("âš ï¸  éƒ¨åˆ†ä»»åŠ¡å¯èƒ½ä»åœ¨å¤„ç†ä¸­æˆ–è¶…æ—¶")
    
    # 6. æµ‹è¯•ç»Ÿè®¡ä¿¡æ¯
    print("\nğŸ“‹ æ­¥éª¤ 6: æŸ¥çœ‹åè°ƒå™¨ç»Ÿè®¡ä¿¡æ¯")
    stats = await coordinator.get_coordinator_stats()
    print("âœ… åè°ƒå™¨ç»Ÿè®¡:")
    print(f"   æ´»è·ƒä»»åŠ¡: {stats['active_tasks']}")
    print(f"   æ€»ä»»åŠ¡æ•°: {stats['total_tasks']}")
    print(f"   æˆåŠŸç‡: {stats['success_rate']:.2%}")
    print(f"   å¹³å‡å¤„ç†æ—¶é—´: {stats['avg_processing_time']:.2f}s")
    print(f"   å¹³å‡æˆæœ¬: ${stats['avg_cost']:.6f}")
    print(f"   æ³¨å†Œæ¨¡å‹æ•°: {stats['registered_models']}")
    print(f"   æ´»è·ƒæ¨¡å‹æ•°: {stats['active_models']}")
    
    # 7. æµ‹è¯•æ¨¡å‹æ¨è
    print("\nğŸ“‹ æ­¥éª¤ 7: æµ‹è¯•æ¨¡å‹æ¨è")
    recommended_models = coordinator.get_available_models(TaskType.TEXT_GENERATION)
    print(f"âœ… æ–‡æœ¬ç”Ÿæˆä»»åŠ¡æ¨èæ¨¡å‹:")
    for model in recommended_models[:3]:  # æ˜¾ç¤ºå‰3ä¸ª
        print(f"   - {model.model_id}: è´¨é‡ {model.quality_score:.2f}, æˆæœ¬ ${model.cost_per_token:.6f}/token")
    
    # 8. æ¸…ç†
    print("\nğŸ“‹ æ­¥éª¤ 8: æ¸…ç†èµ„æº")
    await coordinator.shutdown()
    print("âœ… åè°ƒå™¨å·²å…³é—­")
    
    print("\nğŸ‰ å¤šæ¨¡å‹åè°ƒå™¨æµ‹è¯•å®Œæˆï¼")
    print("=" * 50)


async def test_model_registration():
    """æµ‹è¯•æ¨¡å‹æ³¨å†ŒåŠŸèƒ½"""
    
    print("\nğŸ”§ æµ‹è¯•æ¨¡å‹æ³¨å†ŒåŠŸèƒ½")
    print("-" * 30)
    
    coordinator = MultiModelCoordinator()
    await coordinator.initialize()
    
    # æµ‹è¯•æ³¨å†Œä¸åŒç±»å‹çš„æ¨¡å‹
    test_models = [
        ModelConfig(
            model_id="claude-2",
            model_name="Claude 2",
            model_type=ModelType.TEXT_UNDERSTANDING,
            provider="anthropic",
            capabilities=[TaskType.TEXT_ANALYSIS, TaskType.REASONING],
            cost_per_token=0.00001,
            quality_score=0.9
        ),
        ModelConfig(
            model_id="codex",
            model_name="OpenAI Codex",
            model_type=ModelType.CODE_GENERATION,
            provider="openai",
            capabilities=[TaskType.CODE_GENERATION, TaskType.TEXT_GENERATION],
            cost_per_token=0.000003,
            quality_score=0.85
        ),
        ModelConfig(
            model_id="local-code-llama",
            model_name="Code Llama 7B",
            model_type=ModelType.CODE_GENERATION,
            provider="local",
            capabilities=[TaskType.CODE_GENERATION],
            cost_per_token=0.0,
            quality_score=0.7
        )
    ]
    
    for model in test_models:
        success = coordinator.register_model(model)
        status = "âœ… æˆåŠŸ" if success else "âŒ å¤±è´¥"
        print(f"{status} æ³¨å†Œæ¨¡å‹: {model.model_id}")
    
    # æ˜¾ç¤ºæ‰€æœ‰æ³¨å†Œæ¨¡å‹
    all_models = list(coordinator.models.values())
    print(f"\næ€»è®¡æ³¨å†Œæ¨¡å‹: {len(all_models)}")
    
    # æŒ‰æä¾›è€…åˆ†ç»„
    providers = {}
    for model in all_models:
        if model.provider not in providers:
            providers[model.provider] = []
        providers[model.provider].append(model)
    
    for provider, models in providers.items():
        print(f"\n{provider} æä¾›è€… ({len(models)} ä¸ªæ¨¡å‹):")
        for model in models:
            print(f"  - {model.model_id}: {model.model_name}")
    
    await coordinator.shutdown()


async def test_task_cancellation():
    """æµ‹è¯•ä»»åŠ¡å–æ¶ˆåŠŸèƒ½"""
    
    print("\nğŸš« æµ‹è¯•ä»»åŠ¡å–æ¶ˆåŠŸèƒ½")
    print("-" * 30)
    
    coordinator = MultiModelCoordinator()
    await coordinator.initialize()
    
    # æäº¤ä¸€ä¸ªé•¿æ—¶é—´ä»»åŠ¡
    long_task = TaskRequest(
        task_id="long_task_001",
        task_type=TaskType.TEXT_ANALYSIS,
        content="è¯·è¯¦ç»†åˆ†æä»¥ä¸‹é•¿æ–‡æœ¬ï¼š" + "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æœ¬ã€‚ " * 100,  # æ¨¡æ‹Ÿé•¿æ–‡æœ¬
        priority=TaskPriority.NORMAL,
        required_capabilities=[TaskType.TEXT_ANALYSIS],
        max_time=60  # å…è®¸60ç§’å¤„ç†æ—¶é—´
    )
    
    task_id = await coordinator.submit_task(long_task)
    print(f"âœ… é•¿æ—¶é—´ä»»åŠ¡å·²æäº¤: {task_id}")
    
    # ç«‹å³å–æ¶ˆä»»åŠ¡
    await asyncio.sleep(1)  # ç­‰å¾…ä»»åŠ¡å¼€å§‹å¤„ç†
    success = await coordinator.cancel_task(task_id)
    
    if success:
        print("âœ… ä»»åŠ¡å–æ¶ˆæˆåŠŸ")
        
        # æ£€æŸ¥ä»»åŠ¡çŠ¶æ€
        result = await coordinator.get_task_result(task_id)
        if result:
            print(f"   æœ€ç»ˆçŠ¶æ€: {result.status.value}")
    else:
        print("âŒ ä»»åŠ¡å–æ¶ˆå¤±è´¥")
    
    await coordinator.shutdown()


async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    try:
        await test_multi_model_coordinator()
        await test_model_registration()
        await test_task_cancellation()
        
        print("\nğŸ¯ æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(main())